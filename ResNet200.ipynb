{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet200.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1Q5eLWlyxdyWPEVvqN-lzkK8IKLlxFi7X",
      "authorship_tag": "ABX9TyON1k8FTumkkAAlB6vyPFt3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/By0ungJoo/dacon/blob/main/ResNet200.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import shutil # high-level file operations\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import re\n",
        "import cv2\n",
        "import gc\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.initializers import glorot_uniform"
      ],
      "metadata": {
        "id": "CQ55w08oudxF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "## 주어진 train_df에 각 레이블마다 원-핫인코딩값을 생성할 인덱스 부여\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/DACON_이상치 탐지 알고리즘 경진대회/open/train_df.csv')\n",
        "final_train88_df = train_df.copy()\n",
        "label_lst = final_train88_df.label.unique().tolist()\n",
        "label_lst.sort()\n",
        "one_hot_label = pd.DataFrame(label_lst, columns = {'label'})\n",
        "one_hot_label['one_hot_label'] = one_hot_label.index.tolist()\n",
        "one_hot_label\n",
        "\n",
        "final_train88_df = pd.merge(final_train88_df, one_hot_label, how = 'left', on = 'label')\n",
        "final_train88_df\n",
        "\n",
        "# 학습 파일 기본 경로 지정 및 넘파이 배열로 변환\n",
        "# 넘파일 배열을 생성할 레이블의 one_hot_label값 [0 ~ 87] / df > one_hot_label 참조\n",
        "image_dir = '/content/drive/MyDrive/dacon/dacon/open/train'\n",
        "num_total_label = 88 # 레이블 총 개수\n",
        "one_hot_label = [x for x in range(88)]\n",
        "\n",
        "img_rows = 224\n",
        "img_cols = 224\n",
        "\n",
        "X = [] # 입력 데이터\n",
        "Y = [] # 정답값\n",
        "\n",
        "## 이미지 배열 파일 생성\n",
        "for index, row in tqdm(final_train88_df.iterrows(), total = len(final_train88_df)) : \n",
        "  filename = row['file_name']\n",
        "  label_idx = row['one_hot_label']\n",
        "  # img_label : Y 배열에 들어갈 원-핫 인코딩된 정답값\n",
        "  img_label = [0 for i in range(num_total_label)]\n",
        "  img_label[label_idx] = 1\n",
        "\n",
        "  img = cv2.imread(f'{image_dir}/{filename}').astype('float32')\n",
        "  img = cv2.resize(img, (img_rows,img_cols))#, fx=img_w/img.shape[1], fy=img_h/img.shape[0])\n",
        "  X.append(img/256)\n",
        "  Y.append(img_label)\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.3, random_state=1234)\n",
        "\n",
        "## 안쓰는 변수 비우기 : 램관리\n",
        "del X, Y\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeK3x-wOrPCK",
        "outputId": "86b076cd-78be-4d0b-e9d2-c7342de9847f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4277/4277 [37:36<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bottleneck_residual_block(X, f, filters, stage, block, reduce=False, s=2):\n",
        "    \"\"\"    \n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, height, width, channels)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    \n",
        "    reduce -- boolean, True = identifies the reduction layer at the beginning of each learning stage\n",
        "    s -- integer, strides\n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (H, W, C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value. You'll need this later to add back to the main path. \n",
        "    X_shortcut = X\n",
        "    \n",
        "    if reduce:\n",
        "        # if we are to reduce the spatial size, apply a 1x1 CONV layer to the shortcut path\n",
        "        # to do that, we need both CONV layers to have similar strides \n",
        "        X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "        X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "        X = Activation('relu')(X)\n",
        "        \n",
        "        X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1',\n",
        "                        kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "        X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
        "    else: \n",
        "        # First component of main path\n",
        "        X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "        X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "        X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation \n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "metadata": {
        "id": "NfwzObjs_J89"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet50(input_shape, classes):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    input_shape -- tuple shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
        "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = bottleneck_residual_block(X, 3, [64, 64, 256], stage=2, block='a', reduce=True, s=1)\n",
        "    X = bottleneck_residual_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = bottleneck_residual_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3 \n",
        "    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='a', reduce=True, s=2)\n",
        "    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='e')\n",
        "    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='f')\n",
        "    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='g')\n",
        "    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='h')\n",
        "    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='i')\n",
        "\n",
        "    # Stage 4 \n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='a', reduce=True, s=2)\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='g')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='h')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='i')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='j')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='k')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='l')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='m')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='n')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='o')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='p')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='q')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='r')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='s')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='t')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='u')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='v')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='w')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='x')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='y')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='z')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='aa')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='ab')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='ac')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='ad')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='ae')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='af')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='ag')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='ah')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='ai')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='aj')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='ak')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='al')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='am')\n",
        "    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='an')\n",
        "\n",
        "    # Stage 5 \n",
        "    X = bottleneck_residual_block(X, 3, [512, 512, 2048], stage=5, block='a', reduce=True, s=2)\n",
        "    X = bottleneck_residual_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = bottleneck_residual_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL \n",
        "    X = AveragePooling2D((1,1), name=\"avg_pool\")(X)\n",
        "\n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    # Create the model\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "rn5OUSFe_NLu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet50(input_shape = (224, 224, 3), classes = 88)"
      ],
      "metadata": {
        "id": "2v_Riaxr_REK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "F25AQKOysd2t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath='/content/drive/MyDrive/dacon/dacon/model1.h5', monitor='loss', save_best_only=True)\n",
        "# patience 20\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "VMthUMvxsglM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 64, epoch = 100\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=100, validation_data=(x_test, y_test), callbacks =[checkpoint_cb, early_stopping_cb])"
      ],
      "metadata": {
        "id": "y4DCwioxsgiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53597a8-8c12-43d1-e45a-a50d4b05189b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "47/47 [==============================] - 718s 15s/step - loss: 15.1987 - accuracy: 0.1323 - val_loss: 3.6206 - val_accuracy: 0.0576\n",
            "Epoch 2/100\n",
            "47/47 [==============================] - 594s 13s/step - loss: 4.3663 - accuracy: 0.4450 - val_loss: 4.0826 - val_accuracy: 0.0576\n",
            "Epoch 3/100\n",
            "47/47 [==============================] - 564s 12s/step - loss: 3.6890 - accuracy: 0.7267 - val_loss: 9976.8330 - val_accuracy: 0.0654\n",
            "Epoch 4/100\n",
            "47/47 [==============================] - 567s 12s/step - loss: 1.0537 - accuracy: 0.7691 - val_loss: 6.3131 - val_accuracy: 0.1324\n",
            "Epoch 5/100\n",
            "47/47 [==============================] - 582s 12s/step - loss: 2.2749 - accuracy: 0.7758 - val_loss: 12.4272 - val_accuracy: 0.0654\n",
            "Epoch 6/100\n",
            "47/47 [==============================] - 581s 12s/step - loss: 2.0369 - accuracy: 0.7812 - val_loss: 3.3716 - val_accuracy: 0.2477\n",
            "Epoch 7/100\n",
            "47/47 [==============================] - 600s 13s/step - loss: 0.7998 - accuracy: 0.8219 - val_loss: 3.2777 - val_accuracy: 0.2967\n",
            "Epoch 8/100\n",
            "47/47 [==============================] - 592s 13s/step - loss: 0.7007 - accuracy: 0.8443 - val_loss: 3.2100 - val_accuracy: 0.3629\n",
            "Epoch 9/100\n",
            "47/47 [==============================] - 569s 12s/step - loss: 0.9970 - accuracy: 0.8339 - val_loss: 2.2077 - val_accuracy: 0.6223\n",
            "Epoch 10/100\n",
            "47/47 [==============================] - 586s 12s/step - loss: 1.5432 - accuracy: 0.8303 - val_loss: 789362.5625 - val_accuracy: 0.0639\n",
            "Epoch 11/100\n",
            "47/47 [==============================] - 572s 12s/step - loss: 1.5795 - accuracy: 0.8319 - val_loss: 1023.8373 - val_accuracy: 0.5039\n",
            "Epoch 12/100\n",
            "47/47 [==============================] - 580s 12s/step - loss: 1.3087 - accuracy: 0.8366 - val_loss: 2.2278 - val_accuracy: 0.4868\n",
            "Epoch 13/100\n",
            "47/47 [==============================] - 573s 12s/step - loss: 3.9833 - accuracy: 0.7548 - val_loss: 1479.5767 - val_accuracy: 0.0911\n",
            "Epoch 14/100\n",
            "47/47 [==============================] - 596s 13s/step - loss: 1.3539 - accuracy: 0.8035 - val_loss: 6.5263 - val_accuracy: 0.1713\n",
            "Epoch 15/100\n",
            "47/47 [==============================] - 586s 12s/step - loss: 0.8788 - accuracy: 0.8360 - val_loss: 4.3881 - val_accuracy: 0.4992\n",
            "Epoch 16/100\n",
            "47/47 [==============================] - 584s 12s/step - loss: 0.7438 - accuracy: 0.8430 - val_loss: 2.4650 - val_accuracy: 0.5810\n",
            "Epoch 17/100\n",
            "47/47 [==============================] - 597s 13s/step - loss: 0.6540 - accuracy: 0.8470 - val_loss: 1.4627 - val_accuracy: 0.6947\n",
            "Epoch 18/100\n",
            "47/47 [==============================] - 590s 13s/step - loss: 0.6516 - accuracy: 0.8503 - val_loss: 1.0334 - val_accuracy: 0.7539\n",
            "Epoch 19/100\n",
            "47/47 [==============================] - 581s 12s/step - loss: 0.5913 - accuracy: 0.8557 - val_loss: 0.6800 - val_accuracy: 0.8497\n",
            "Epoch 20/100\n",
            "47/47 [==============================] - 732s 16s/step - loss: 0.6217 - accuracy: 0.8523 - val_loss: 0.8354 - val_accuracy: 0.7936\n",
            "Epoch 21/100\n",
            "47/47 [==============================] - 681s 14s/step - loss: 0.5690 - accuracy: 0.8577 - val_loss: 0.6688 - val_accuracy: 0.8497\n",
            "Epoch 22/100\n",
            "47/47 [==============================] - 578s 12s/step - loss: 0.5354 - accuracy: 0.8664 - val_loss: 0.6917 - val_accuracy: 0.8388\n",
            "Epoch 23/100\n",
            "47/47 [==============================] - 590s 13s/step - loss: 0.5014 - accuracy: 0.8710 - val_loss: 0.6640 - val_accuracy: 0.8450\n",
            "Epoch 24/100\n",
            "47/47 [==============================] - 593s 13s/step - loss: 0.4818 - accuracy: 0.8747 - val_loss: 0.6402 - val_accuracy: 0.8505\n",
            "Epoch 25/100\n",
            "47/47 [==============================] - 582s 12s/step - loss: 0.4755 - accuracy: 0.8744 - val_loss: 0.6844 - val_accuracy: 0.8427\n",
            "Epoch 26/100\n",
            "47/47 [==============================] - 589s 13s/step - loss: 0.4413 - accuracy: 0.8844 - val_loss: 0.6546 - val_accuracy: 0.8512\n",
            "Epoch 27/100\n",
            "47/47 [==============================] - 684s 15s/step - loss: 0.4316 - accuracy: 0.8857 - val_loss: 0.6809 - val_accuracy: 0.8505\n",
            "Epoch 28/100\n",
            "47/47 [==============================] - 679s 14s/step - loss: 0.4034 - accuracy: 0.8944 - val_loss: 0.7084 - val_accuracy: 0.8512\n",
            "Epoch 29/100\n",
            "47/47 [==============================] - 667s 14s/step - loss: 0.3922 - accuracy: 0.8938 - val_loss: 0.7130 - val_accuracy: 0.8427\n",
            "Epoch 30/100\n",
            "47/47 [==============================] - 656s 14s/step - loss: 0.3906 - accuracy: 0.8904 - val_loss: 0.7700 - val_accuracy: 0.8435\n",
            "Epoch 31/100\n",
            "26/47 [===============>..............] - ETA: 4:16 - loss: 0.3395 - accuracy: 0.9038"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 학습 과정 살펴보기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## 그래프 : loss , val_loss\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "## loss 그래프\n",
        "loss_ax.plot(history.history['loss'],'y',label='train_loss') # 노란색\n",
        "loss_ax.plot(history.history['val_loss'],color='red' ,label='val_loss') # 빨간색\n",
        "\n",
        "## accuracy 그래프\n",
        "acc_ax.plot(history.history['accuracy'],'g',label='train_accuracy') # 초록색\n",
        "acc_ax.plot(history.history['val_accuracy'],'b',label='val_accuracy') # 파란색\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3bf3-Iixsgdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch 100 으로 코드 돌리기 "
      ],
      "metadata": {
        "id": "5VF5pQaz97-2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}