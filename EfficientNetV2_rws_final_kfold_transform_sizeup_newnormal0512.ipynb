{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/By0ungJoo/dacon/blob/main/EfficientNetV2_rws_final_kfold_transform_sizeup_newnormal0512.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lynhGDxilbRs"
      },
      "source": [
        "## efficientNet_b4 + 드롭아웃 0.3 + 옵티마이저 sam + kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKXv22Yk6zus",
        "outputId": "719c6aa0-b55c-4cb2-ea40-f4319a347e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTeQpqhWZXL1"
      },
      "outputs": [],
      "source": [
        "# !pip install albumentations==0.4.6\n",
        "# !pip install --upgrade --force-reinstall --no-deps albumentations\n",
        "\n",
        "# from albumentations.pytorch.transforms import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_MioD9UThpP",
        "outputId": "681631fa-c5ab-4651-e671-02167c9f70ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 117 kB 9.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 948 kB 72.2 MB/s \n",
            "\u001b[?25h  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "albumentations==0.4.6 is successfully installed\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U albumentations==0.4.6\n",
        "!echo \"$(pip freeze | grep albumentations) is successfully installed\"\n",
        "\n",
        "from albumentations.pytorch.transforms import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjs-R_R5lUKg",
        "outputId": "dad92a2a-367d-4d43-c5a9-195ecdf488f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 31.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 17.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ],
      "source": [
        "## 학습 코드\n",
        "!pip install timm\n",
        "import timm\n",
        "\n",
        "# from pprint import pprint\n",
        "# model_names = timm.list_models(pretrained=True)\n",
        "# pprint(model_names)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "#import timm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "#!pip install albumentations==0.4.6\n",
        "import albumentations\n",
        "#from albumentations.pytorch import ToTensorV2\n",
        "# from efficientnet_pytorch import EfficientNet\n",
        "# model_name = 'efficientnet-b0'  # b5\n",
        "\n",
        "# image_size = EfficientNet.get_image_size(model_name)\n",
        "# print(image_size)\n",
        "# model = EfficientNet.from_pretrained(model_name, num_classes=88)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZHMCtAvlDOp"
      },
      "outputs": [],
      "source": [
        "## 기본 설정\n",
        "device = torch.device('cuda')\n",
        "batch_size  = 32\n",
        "random_seed = 1234\n",
        "img_size = 300\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "def main(seed = random_seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "main(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTgH3bYzN7RA"
      },
      "source": [
        "## transforms 함수 정리\n",
        "* transforms.ToPILImage() - csv 파일로 데이터셋을 받을 경우, PIL image로 바꿔준다.\n",
        "* transforms.CenterCrop(size) - 가운데 부분을 size 크기로 자른다.\n",
        "* transforms.Grayscale(num_output_channels=1) - grayscale로 변환한다.\n",
        "* transforms.RandomAffine(degrees) - 랜덤으로 affine 변형을 한다.\n",
        "* transforms.RandomCrop(size) -이미지를 랜덤으로 아무데나 잘라 size 크기로 출력한다.\n",
        "* transforms.RandomResizedCrop(size) - 이미지 사이즈를 size로 변경한다\n",
        "* transforms.Resize(size) - 이미지 사이즈를 size로 변경한다\n",
        "* transforms.RandomRotation(degrees) 이미지를 랜덤으로 degrees 각도로 회전한다.\n",
        "* transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333)) - 이미지를 랜덤으로 변형한다.\n",
        "* transforms.RandomVerticalFlip(p=0.5) - 이미지를 랜덤으로 수직으로 뒤집는다. p =0이면 뒤집지 않는다.\n",
        "* transforms.RandomHorizontalFlip(p=0.5) - 이미지를 랜덤으로 수평으로 뒤집는다.\n",
        "* transforms.ToTensor() - 이미지 데이터를 tensor로 바꿔준다.\n",
        "* transforms.Normalize(mean, std, inplace=False) - 이미지를 정규화한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORqFbwLXpSeu"
      },
      "outputs": [],
      "source": [
        "def img_load(path):\n",
        "    img = cv2.imread(path)[:,:,::-1] # Return type:\tnumpy.ndarray / 기본적으로 BGR로 불러옴, RGB 변환함\n",
        "    img = cv2.resize(img, (300, 300))\n",
        "    return img\n",
        "\n",
        "class Custom_dataset_3(Dataset): # 기본이미지 + 변형이미지 +오류데이터*3\n",
        "    def __init__(self, img_paths, labels, mode='train'):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.mode=mode\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode=='train':\n",
        "          if idx < 4277: # 변경 없는 이미지\n",
        "            img = self.img_paths[idx]\n",
        "            img = cv2.imread(path)[:,:,::-1]\n",
        "            img = cv2.resize(img, (300, 300))\n",
        "            img = transforms.ToTensor()(img) # PIL or ndarray -> tensor\n",
        "            img = transforms.Normalize(mean=[0.433, 0.405, 0.396], std=[0.183, 0.177, 0.166])(img)\n",
        "            label = self.labels[idx]\n",
        "            return img, label\n",
        "\n",
        "          else : # 랜덤 변경된 이미지 추가됨 \n",
        "            img = self.img_paths[idx]\n",
        "            img = Image.fromarray(img)\n",
        "            #img = transforms.Resize((img_size, img_size))(img) # 처음 이미지 불러올때 resize 했으므로 생략\n",
        "            #img = transforms.RandomCrop(img_size)(img)\n",
        "            img = transforms.RandomRotation(90, expand=False)(img)\n",
        "            img = transforms.RandomVerticalFlip()(img)\n",
        "            img = transforms.RandomHorizontalFlip()(img)\n",
        "            img = transforms.ToTensor()(img) # PIL or ndarray -> tensor\n",
        "            img = transforms.Normalize(mean=[0.433, 0.405, 0.396], std=[0.183, 0.177, 0.166])(img)\n",
        "            label = self.labels[idx]\n",
        "            return img, label\n",
        "\n",
        "        if self.mode=='test':\n",
        "          img = self.img_paths[idx]\n",
        "          img = cv2.imread(path)[:,:,::-1]\n",
        "          img = cv2.resize(img, (300, 300))\n",
        "          img = transforms.ToTensor()(img)\n",
        "          img = transforms.Normalize(mean=[0.433, 0.405, 0.396], std=[0.183, 0.177, 0.166])(img)\n",
        "          label = self.labels[idx]\n",
        "          return img, label\n",
        "\n",
        "class Custom_dataset_Albumentations(Dataset):\n",
        "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
        "    def __init__(self, img_list, labels, transform=None, mode='train'):\n",
        "        self.img_list = img_list\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.mode=mode\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        img   = self.img_list[idx]\n",
        "\n",
        "        if self.mode=='train':\n",
        "          if idx < 4277: # 변경 없는 이미지\n",
        "            img = transforms.ToTensor()(img) # PIL or ndarray -> tensor\n",
        "            img = transforms.Normalize(mean=[0.433, 0.405, 0.396], std=[0.183, 0.177, 0.166])(img)\n",
        "\n",
        "          else : # 랜덤 변경된 이미지 추가됨 \n",
        "            if self.transform:\n",
        "                #img = Image.fromarray(img)\n",
        "                augmented = self.transform(image=img)\n",
        "                img = augmented['image']\n",
        "\n",
        "        if self.mode=='test':\n",
        "          img = transforms.ToTensor()(img)\n",
        "          img = transforms.Normalize(mean=[0.433, 0.405, 0.396], std=[0.183, 0.177, 0.166])(img)\n",
        "        return img, label\n",
        "\n",
        "albumentations_transform = albumentations.Compose([albumentations.HorizontalFlip(p=0.5),\n",
        "                                                   albumentations.VerticalFlip(p=0.5),\n",
        "                                                   albumentations.ShiftScaleRotate(shift_limit=(-0.1, 0.1), scale_limit=(0.0, 0.3), rotate_limit=[-45, 45], p=0.5, border_mode=cv2.INTER_NEAREST),\n",
        "                                                   albumentations.Normalize(mean=[0.433, 0.405, 0.396], std=[0.183, 0.177, 0.166],),\n",
        "                                                   albumentations.pytorch.ToTensorV2()])\n",
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                e_w = p.grad * scale.to(p)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "                self.state[p][\"e_w\"] = e_w\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        p.grad.norm(p=2).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm\n",
        "import math\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
        "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
        "        if T_0 <= 0 or not isinstance(T_0, int):\n",
        "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
        "        if T_mult < 1 or not isinstance(T_mult, int):\n",
        "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
        "        if T_up < 0 or not isinstance(T_up, int):\n",
        "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
        "        self.T_0 = T_0\n",
        "        self.T_mult = T_mult\n",
        "        self.base_eta_max = eta_max\n",
        "        self.eta_max = eta_max\n",
        "        self.T_up = T_up\n",
        "        self.T_i = T_0\n",
        "        self.gamma = gamma\n",
        "        self.cycle = 0\n",
        "        self.T_cur = last_epoch\n",
        "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
        "    \n",
        "    def get_lr(self):\n",
        "        if self.T_cur == -1:\n",
        "            return self.base_lrs\n",
        "        elif self.T_cur < self.T_up:\n",
        "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.T_cur = self.T_cur + 1\n",
        "            if self.T_cur >= self.T_i:\n",
        "                self.cycle += 1\n",
        "                self.T_cur = self.T_cur - self.T_i\n",
        "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
        "        else:\n",
        "            if epoch >= self.T_0:\n",
        "                if self.T_mult == 1:\n",
        "                    self.T_cur = epoch % self.T_0\n",
        "                    self.cycle = epoch // self.T_0\n",
        "                else:\n",
        "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
        "                    self.cycle = n\n",
        "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
        "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
        "            else:\n",
        "                self.T_i = self.T_0\n",
        "                self.T_cur = epoch\n",
        "                \n",
        "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self,mode = 'train'):\n",
        "        super(Network, self).__init__()\n",
        "        self.mode = mode\n",
        "        if self.mode == 'train':\n",
        "          self.model = timm.create_model('efficientnetv2_rw_s', pretrained=True, num_classes=88, drop_path_rate = 0.3)\n",
        "        if self.mode == 'test':\n",
        "          self.model = timm.create_model('efficientnetv2_rw_s', pretrained=True, num_classes=88, drop_path_rate = 0)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "### f1 스코어 함수 \n",
        "def score_function(real, pred): # 라이브러리 > sklearn.metrics \n",
        "    score = f1_score(real, pred, average=\"macro\")\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItvLP1Y3k-OW",
        "outputId": "498ea7cb-7615-425d-bc5f-36c042c18cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4277/4277 [04:28<00:00, 15.93it/s]\n",
            "100%|██████████| 4277/4277 [00:00<00:00, 1752543.79it/s]\n",
            "100%|██████████| 648/648 [00:19<00:00, 33.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10498 10498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "## 이미지 로드 및 전처리\n",
        "train_imgs = sorted(glob('/content/drive/MyDrive/dacon/dacon/open/train1/*.png'))\n",
        "train_imgs = [img_load(x) for x in tqdm(train_imgs)]\n",
        "\n",
        "# train_imgs = np.load('/content/drive/MyDrive/DACON_이상치 탐지 알고리즘 경진대회/train_imgs_224.npy')\n",
        "train_y = pd.read_csv(\"/content/drive/MyDrive/dacon/dacon/open/train_df.csv\")\n",
        "# \n",
        "train_labels = train_y[\"label\"] # 레이블순서는 이미지 파일 순서대로임\n",
        "\n",
        "# 레이블별 키값\n",
        "label_unique = sorted(np.unique(train_labels))\n",
        "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))} # 오름차순으로 레이블별로 숫자 부여(0부터 시작)\n",
        "\n",
        "train_labels = [label_unique[k] for k in tqdm(train_labels)]\n",
        "\n",
        "# not_good ## \n",
        "not_good_imgs = [f'/content/drive/MyDrive/dacon/dacon/open/train1/{x}' for x in train_y[train_y.state != 'good'].file_name.tolist()]\n",
        "not_good_imgs = [img_load(x) for x in tqdm(not_good_imgs)]\n",
        "not_good_label = train_y[train_y.state != 'good'].label.tolist()\n",
        "not_good_labels = [label_unique[k] for k in not_good_label]\n",
        "\n",
        "### 총 이미지 리스트 생성 ### 기본 + 증량 + 오류*3 = 10498\n",
        "\n",
        "train_imgs   = (train_imgs * 2) + (not_good_imgs * 3) # 이미지 리스트\n",
        "train_labels = (train_labels * 2) + (not_good_labels * 3)  # 숫자 레이블\n",
        "\n",
        "print(len(train_imgs), len(train_labels))\n",
        "#### \n",
        "## 이미지 전처리 시작\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcwsPLMa2Ym5"
      },
      "outputs": [],
      "source": [
        "# train_dataset = Custom_dataset_Albumentations(img_list = train_imgs,\n",
        "#                                               labels     = train_labels,\n",
        "#                                               transform  = albumentations_transform,\n",
        "#                                               mode = 'train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGpqV_vBBlJF"
      },
      "outputs": [],
      "source": [
        "## 데이터셋 분리 > 층화추출 & 테스터 데이터 비율 : 0.3, 시드 : 1234, 셔플 = True(default)\n",
        "# train_idx, vaild_idx = train_test_split(list(range(len(train_dataset))), stratify=train_labels, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "# datasets = {} # 데이터셋을 담을 딕셔너리\n",
        "# datasets['train'] = Subset(train_dataset, train_idx)\n",
        "# datasets['vaild']  = Subset(train_dataset, vaild_idx)\n",
        "\n",
        "# ## data loader 선언\n",
        "# dataloaders, batch_num = {}, {}\n",
        "# dataloaders['train'] = torch.utils.data.DataLoader(datasets['train'],\n",
        "#                                                   batch_size=batch_size, shuffle=True,\n",
        "#                                                   num_workers=0)\n",
        "# dataloaders['vaild']  = torch.utils.data.DataLoader(datasets['vaild'],\n",
        "#                                                     batch_size=batch_size, shuffle=False,\n",
        "#                                                     num_workers=0)\n",
        "\n",
        "# batch_num['train'], batch_num['vaild'] = len(dataloaders['train']), len(dataloaders['vaild'])\n",
        "# print('batch_size : %d,  train/vaild(데이터셋개수/배치사이즈) : %d / %d' % (batch_size, batch_num['train'],batch_num['vaild']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1usE9wa91TX",
        "outputId": "9b6648bc-898d-4704-b611-c68f2da209d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "efficientnetv2_rw_s_transform_size_up_normal\n"
          ]
        }
      ],
      "source": [
        "model_version = str(input()) # efficientNet_b4_ver3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB7x9q-ux0ER"
      },
      "outputs": [],
      "source": [
        "efficientnetv2_rw_s_transform_size_up_normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDNj4Yarkx-R",
        "outputId": "38725ca4-3d0f-492e-a927-2a9e2f563caf"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------0 fold start!----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_v2s_ra2_288-a6477665.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2s_ra2_288-a6477665.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### KFold 0 / 1 ###\n",
            "Epoch 0/29\n",
            "----------\n",
            "train Loss: 4.60015 Acc: 0.62869 macro-f1: 0.00446\n",
            "vaild Loss: 4.54319 Acc: 0.49533 macro-f1: 0.00465\n",
            "Accumulate epochs training time : 2m 55s\n",
            "\n",
            "\n",
            "Epoch 1/29\n",
            "----------\n",
            "train Loss: 1.23520 Acc: 72.31854 macro-f1: 0.32048\n",
            "vaild Loss: 0.65361 Acc: 81.82511 macro-f1: 0.43381\n",
            "Accumulate epochs training time : 5m 48s\n",
            "\n",
            "\n",
            "Epoch 2/29\n",
            "----------\n",
            "train Loss: 0.69588 Acc: 80.33911 macro-f1: 0.53152\n",
            "vaild Loss: 0.48738 Acc: 85.73062 macro-f1: 0.58373\n",
            "Accumulate epochs training time : 8m 40s\n",
            "\n",
            "\n",
            "Epoch 3/29\n",
            "----------\n",
            "train Loss: 0.58976 Acc: 82.85388 macro-f1: 0.61090\n",
            "vaild Loss: 0.49285 Acc: 87.69289 macro-f1: 0.66098\n",
            "Accumulate epochs training time : 11m 33s\n",
            "\n",
            "\n",
            "Epoch 4/29\n",
            "----------\n",
            "train Loss: 0.66654 Acc: 81.25357 macro-f1: 0.58140\n",
            "vaild Loss: 0.55576 Acc: 84.18746 macro-f1: 0.59017\n",
            "Accumulate epochs training time : 14m 26s\n",
            "\n",
            "\n",
            "Epoch 5/29\n",
            "----------\n",
            "train Loss: 0.41814 Acc: 87.10231 macro-f1: 0.70997\n",
            "vaild Loss: 0.32421 Acc: 89.86474 macro-f1: 0.74030\n",
            "Accumulate epochs training time : 17m 18s\n",
            "\n",
            "\n",
            "Epoch 6/29\n",
            "----------\n",
            "train Loss: 0.25884 Acc: 91.84607 macro-f1: 0.81777\n",
            "vaild Loss: 0.24861 Acc: 92.60812 macro-f1: 0.82215\n",
            "Accumulate epochs training time : 20m 11s\n",
            "\n",
            "\n",
            "Epoch 7/29\n",
            "----------\n",
            "train Loss: 0.20592 Acc: 93.63688 macro-f1: 0.85444\n",
            "vaild Loss: 0.28625 Acc: 92.07468 macro-f1: 0.84408\n",
            "Accumulate epochs training time : 23m 4s\n",
            "\n",
            "\n",
            "Epoch 8/29\n",
            "----------\n",
            "train Loss: 0.18643 Acc: 94.07506 macro-f1: 0.86982\n",
            "vaild Loss: 0.24208 Acc: 93.35111 macro-f1: 0.86084\n",
            "Accumulate epochs training time : 25m 56s\n",
            "\n",
            "\n",
            "Epoch 9/29\n",
            "----------\n",
            "train Loss: 0.16908 Acc: 94.64660 macro-f1: 0.88100\n",
            "vaild Loss: 0.19741 Acc: 94.13222 macro-f1: 0.87713\n",
            "Accumulate epochs training time : 28m 49s\n",
            "\n",
            "\n",
            "Epoch 10/29\n",
            "----------\n",
            "train Loss: 0.15802 Acc: 95.37055 macro-f1: 0.90205\n",
            "vaild Loss: 0.15569 Acc: 95.25624 macro-f1: 0.89981\n",
            "Accumulate epochs training time : 31m 42s\n",
            "\n",
            "\n",
            "Epoch 11/29\n",
            "----------\n",
            "train Loss: 0.08205 Acc: 97.31377 macro-f1: 0.94087\n",
            "vaild Loss: 0.12369 Acc: 96.07544 macro-f1: 0.92918\n",
            "Accumulate epochs training time : 34m 35s\n",
            "\n",
            "\n",
            "Epoch 12/29\n",
            "----------\n",
            "train Loss: 0.06209 Acc: 97.94247 macro-f1: 0.95274\n",
            "vaild Loss: 0.10383 Acc: 97.08516 macro-f1: 0.94557\n",
            "Accumulate epochs training time : 37m 27s\n",
            "\n",
            "\n",
            "Epoch 13/29\n",
            "----------\n",
            "train Loss: 0.05221 Acc: 98.49495 macro-f1: 0.96462\n",
            "vaild Loss: 0.06653 Acc: 97.67575 macro-f1: 0.95758\n",
            "Accumulate epochs training time : 40m 20s\n",
            "\n",
            "\n",
            "Epoch 14/29\n",
            "----------\n",
            "train Loss: 0.03978 Acc: 98.72357 macro-f1: 0.97167\n",
            "vaild Loss: 0.06572 Acc: 97.84721 macro-f1: 0.96372\n",
            "Accumulate epochs training time : 43m 13s\n",
            "\n",
            "\n",
            "Epoch 15/29\n",
            "----------\n",
            "train Loss: 0.03468 Acc: 98.97123 macro-f1: 0.97736\n",
            "vaild Loss: 0.07011 Acc: 97.94247 macro-f1: 0.96759\n",
            "Accumulate epochs training time : 46m 5s\n",
            "\n",
            "\n",
            "Epoch 16/29\n",
            "----------\n",
            "train Loss: 0.02625 Acc: 99.23795 macro-f1: 0.98258\n",
            "vaild Loss: 0.04146 Acc: 98.43780 macro-f1: 0.97654\n",
            "Accumulate epochs training time : 48m 58s\n",
            "\n",
            "\n",
            "Epoch 17/29\n",
            "----------\n",
            "train Loss: 0.02351 Acc: 99.27605 macro-f1: 0.98445\n",
            "vaild Loss: 0.04247 Acc: 98.62831 macro-f1: 0.97842\n",
            "Accumulate epochs training time : 51m 51s\n",
            "\n",
            "\n",
            "Epoch 18/29\n",
            "----------\n",
            "train Loss: 0.02443 Acc: 99.21890 macro-f1: 0.98085\n",
            "vaild Loss: 0.04673 Acc: 98.53305 macro-f1: 0.97549\n",
            "Accumulate epochs training time : 54m 44s\n",
            "\n",
            "\n",
            "Epoch 19/29\n",
            "----------\n",
            "train Loss: 0.01913 Acc: 99.48562 macro-f1: 0.98810\n",
            "vaild Loss: 0.05660 Acc: 98.36159 macro-f1: 0.97656\n",
            "==> best model saved - 17 / 98.6\n",
            "Accumulate epochs training time : 57m 39s\n",
            "\n",
            "\n",
            "Epoch 20/29\n",
            "----------\n",
            "train Loss: 0.01871 Acc: 99.50467 macro-f1: 0.98918\n",
            "vaild Loss: 0.05082 Acc: 98.43780 macro-f1: 0.97835\n",
            "Accumulate epochs training time : 60m 32s\n",
            "\n",
            "\n",
            "Epoch 21/29\n",
            "----------\n",
            "train Loss: 0.02359 Acc: 99.39036 macro-f1: 0.98542\n",
            "vaild Loss: 0.04648 Acc: 98.51400 macro-f1: 0.97779\n",
            "Accumulate epochs training time : 63m 25s\n",
            "\n",
            "\n",
            "Epoch 22/29\n",
            "----------\n",
            "train Loss: 0.02333 Acc: 99.29510 macro-f1: 0.98382\n",
            "vaild Loss: 0.05751 Acc: 98.24729 macro-f1: 0.97371\n",
            "Accumulate epochs training time : 66m 18s\n",
            "\n",
            "\n",
            "Epoch 23/29\n",
            "----------\n",
            "train Loss: 0.02156 Acc: 99.42846 macro-f1: 0.98769\n",
            "vaild Loss: 0.05344 Acc: 98.43780 macro-f1: 0.97624\n",
            "Accumulate epochs training time : 69m 11s\n",
            "\n",
            "\n",
            "Epoch 24/29\n",
            "----------\n",
            "train Loss: 0.03052 Acc: 99.14269 macro-f1: 0.98004\n",
            "vaild Loss: 0.08338 Acc: 97.38998 macro-f1: 0.96493\n",
            "Accumulate epochs training time : 72m 3s\n",
            "\n",
            "\n",
            "Epoch 25/29\n",
            "----------\n",
            "train Loss: 0.03134 Acc: 98.97123 macro-f1: 0.97826\n",
            "vaild Loss: 0.04371 Acc: 98.47590 macro-f1: 0.97328\n",
            "Accumulate epochs training time : 74m 56s\n",
            "\n",
            "\n",
            "Epoch 26/29\n",
            "----------\n",
            "train Loss: 0.03233 Acc: 99.02839 macro-f1: 0.97761\n",
            "vaild Loss: 0.04941 Acc: 98.70452 macro-f1: 0.97571\n",
            "Accumulate epochs training time : 77m 49s\n",
            "\n",
            "\n",
            "Epoch 27/29\n",
            "----------\n",
            "train Loss: 0.03483 Acc: 98.89503 macro-f1: 0.97643\n",
            "vaild Loss: 0.06819 Acc: 97.84721 macro-f1: 0.96520\n",
            "Accumulate epochs training time : 80m 42s\n",
            "\n",
            "\n",
            "Epoch 28/29\n",
            "----------\n",
            "train Loss: 0.02682 Acc: 99.16175 macro-f1: 0.98024\n",
            "vaild Loss: 0.04320 Acc: 98.62831 macro-f1: 0.97525\n",
            "Accumulate epochs training time : 83m 34s\n",
            "\n",
            "\n",
            "Epoch 29/29\n",
            "----------\n",
            "train Loss: 0.02918 Acc: 99.06649 macro-f1: 0.97871\n",
            "vaild Loss: 0.04239 Acc: 98.55211 macro-f1: 0.97490\n",
            "Accumulate epochs training time : 86m 27s\n",
            "\n",
            "\n",
            "one fold - Training complete in 86m 27s\n",
            "1 Best valid loss & Acc: 26 - 98.7\n",
            "0-Fold model saved\n",
            "Accumulate training complete in 86m 50s\n",
            "----------1 fold start!----------\n",
            "### KFold 1 / 1 ###\n",
            "Epoch 0/29\n",
            "----------\n",
            "train Loss: 4.60068 Acc: 2.45761 macro-f1: 0.01045\n",
            "vaild Loss: 4.51219 Acc: 1.80987 macro-f1: 0.00925\n",
            "Accumulate epochs training time : 2m 51s\n",
            "\n",
            "\n",
            "Epoch 1/29\n",
            "----------\n",
            "train Loss: 1.24747 Acc: 72.07087 macro-f1: 0.31296\n",
            "vaild Loss: 0.75278 Acc: 79.74852 macro-f1: 0.40873\n",
            "Accumulate epochs training time : 5m 43s\n",
            "\n",
            "\n",
            "Epoch 2/29\n",
            "----------\n",
            "train Loss: 0.65644 Acc: 81.00591 macro-f1: 0.54593\n",
            "vaild Loss: 0.58321 Acc: 84.64469 macro-f1: 0.58727\n",
            "Accumulate epochs training time : 8m 34s\n",
            "\n",
            "\n",
            "Epoch 3/29\n",
            "----------\n",
            "train Loss: 0.63673 Acc: 82.16803 macro-f1: 0.59546\n",
            "vaild Loss: 0.51586 Acc: 85.23528 macro-f1: 0.61479\n",
            "Accumulate epochs training time : 11m 26s\n",
            "\n",
            "\n",
            "Epoch 4/29\n",
            "----------\n",
            "train Loss: 0.50915 Acc: 85.23528 macro-f1: 0.66897\n",
            "vaild Loss: 0.43243 Acc: 87.65479 macro-f1: 0.68567\n",
            "Accumulate epochs training time : 14m 17s\n",
            "\n",
            "\n",
            "Epoch 5/29\n",
            "----------\n",
            "train Loss: 0.43222 Acc: 87.02610 macro-f1: 0.70982\n",
            "vaild Loss: 0.41153 Acc: 88.87407 macro-f1: 0.73122\n",
            "Accumulate epochs training time : 17m 8s\n",
            "\n",
            "\n",
            "Epoch 6/29\n",
            "----------\n",
            "train Loss: 0.33363 Acc: 89.48371 macro-f1: 0.76335\n",
            "vaild Loss: 0.26800 Acc: 93.02724 macro-f1: 0.80094\n",
            "Accumulate epochs training time : 19m 60s\n",
            "\n",
            "\n",
            "Epoch 7/29\n",
            "----------\n",
            "train Loss: 0.26598 Acc: 92.18899 macro-f1: 0.82698\n",
            "vaild Loss: 0.24447 Acc: 93.65593 macro-f1: 0.84539\n",
            "Accumulate epochs training time : 22m 51s\n",
            "\n",
            "\n",
            "Epoch 8/29\n",
            "----------\n",
            "train Loss: 0.21390 Acc: 93.67499 macro-f1: 0.86059\n",
            "vaild Loss: 0.18690 Acc: 94.17032 macro-f1: 0.86616\n",
            "Accumulate epochs training time : 25m 43s\n",
            "\n",
            "\n",
            "Epoch 9/29\n",
            "----------\n",
            "train Loss: 0.13009 Acc: 95.88493 macro-f1: 0.90550\n",
            "vaild Loss: 0.19922 Acc: 94.58945 macro-f1: 0.89438\n",
            "Accumulate epochs training time : 28m 34s\n",
            "\n",
            "\n",
            "Epoch 10/29\n",
            "----------\n",
            "train Loss: 0.10588 Acc: 96.45647 macro-f1: 0.92476\n",
            "vaild Loss: 0.12267 Acc: 96.30406 macro-f1: 0.92145\n",
            "Accumulate epochs training time : 31m 26s\n",
            "\n",
            "\n",
            "Epoch 11/29\n",
            "----------\n",
            "train Loss: 0.08008 Acc: 97.38998 macro-f1: 0.94323\n",
            "vaild Loss: 0.13242 Acc: 96.28501 macro-f1: 0.94084\n",
            "Accumulate epochs training time : 34m 17s\n",
            "\n",
            "\n",
            "Epoch 12/29\n",
            "----------\n",
            "train Loss: 0.05967 Acc: 98.13298 macro-f1: 0.95822\n",
            "vaild Loss: 0.09054 Acc: 97.52334 macro-f1: 0.95494\n",
            "Accumulate epochs training time : 37m 8s\n",
            "\n",
            "\n",
            "Epoch 13/29\n",
            "----------\n",
            "train Loss: 0.03705 Acc: 98.89503 macro-f1: 0.97365\n",
            "vaild Loss: 0.10153 Acc: 97.38998 macro-f1: 0.95852\n",
            "Accumulate epochs training time : 39m 59s\n",
            "\n",
            "\n",
            "Epoch 14/29\n",
            "----------\n",
            "train Loss: 0.03469 Acc: 99.04744 macro-f1: 0.97871\n",
            "vaild Loss: 0.05502 Acc: 98.28539 macro-f1: 0.97138\n",
            "Accumulate epochs training time : 42m 51s\n",
            "\n",
            "\n",
            "Epoch 15/29\n",
            "----------\n",
            "train Loss: 0.02805 Acc: 99.18080 macro-f1: 0.98083\n",
            "vaild Loss: 0.06112 Acc: 98.45685 macro-f1: 0.97479\n",
            "Accumulate epochs training time : 45m 42s\n",
            "\n",
            "\n",
            "Epoch 16/29\n",
            "----------\n",
            "train Loss: 0.02208 Acc: 99.37131 macro-f1: 0.98587\n",
            "vaild Loss: 0.04717 Acc: 98.60926 macro-f1: 0.97976\n",
            "Accumulate epochs training time : 48m 33s\n",
            "\n",
            "\n",
            "Epoch 17/29\n",
            "----------\n",
            "train Loss: 0.02296 Acc: 99.35226 macro-f1: 0.98464\n",
            "vaild Loss: 0.04302 Acc: 98.85693 macro-f1: 0.98071\n",
            "Accumulate epochs training time : 51m 25s\n",
            "\n",
            "\n",
            "Epoch 18/29\n",
            "----------\n",
            "train Loss: 0.01911 Acc: 99.48562 macro-f1: 0.98683\n",
            "vaild Loss: 0.03981 Acc: 98.85693 macro-f1: 0.98221\n",
            "Accumulate epochs training time : 54m 16s\n",
            "\n",
            "\n",
            "Epoch 19/29\n",
            "----------\n",
            "train Loss: 0.01757 Acc: 99.56182 macro-f1: 0.98895\n",
            "vaild Loss: 0.06055 Acc: 98.38064 macro-f1: 0.97898\n",
            "==> best model saved - 17 / 98.9\n",
            "Accumulate epochs training time : 57m 9s\n",
            "\n",
            "\n",
            "Epoch 20/29\n",
            "----------\n",
            "train Loss: 0.02077 Acc: 99.50467 macro-f1: 0.98885\n",
            "vaild Loss: 0.04602 Acc: 98.72357 macro-f1: 0.98225\n",
            "Accumulate epochs training time : 59m 60s\n",
            "\n",
            "\n",
            "Epoch 21/29\n",
            "----------\n",
            "train Loss: 0.02011 Acc: 99.52372 macro-f1: 0.98948\n",
            "vaild Loss: 0.05428 Acc: 98.68546 macro-f1: 0.98217\n",
            "Accumulate epochs training time : 62m 51s\n",
            "\n",
            "\n",
            "Epoch 22/29\n",
            "----------\n",
            "train Loss: 0.02356 Acc: 99.44751 macro-f1: 0.98835\n",
            "vaild Loss: 0.04114 Acc: 98.79977 macro-f1: 0.98198\n",
            "Accumulate epochs training time : 65m 42s\n",
            "\n",
            "\n",
            "Epoch 23/29\n",
            "----------\n",
            "train Loss: 0.01898 Acc: 99.40941 macro-f1: 0.98693\n",
            "vaild Loss: 0.05794 Acc: 98.28539 macro-f1: 0.97467\n",
            "Accumulate epochs training time : 68m 34s\n",
            "\n",
            "\n",
            "Epoch 24/29\n",
            "----------\n",
            "train Loss: 0.02562 Acc: 99.29510 macro-f1: 0.98301\n",
            "vaild Loss: 0.06527 Acc: 98.17108 macro-f1: 0.97373\n",
            "Accumulate epochs training time : 71m 25s\n",
            "\n",
            "\n",
            "Epoch 25/29\n",
            "----------\n",
            "train Loss: 0.03010 Acc: 99.04744 macro-f1: 0.97795\n",
            "vaild Loss: 0.05994 Acc: 98.28539 macro-f1: 0.97034\n",
            "Accumulate epochs training time : 74m 16s\n",
            "\n",
            "\n",
            "Epoch 26/29\n",
            "----------\n",
            "train Loss: 0.04657 Acc: 98.59021 macro-f1: 0.96749\n",
            "vaild Loss: 0.05893 Acc: 98.51400 macro-f1: 0.96721\n",
            "Accumulate epochs training time : 77m 8s\n",
            "\n",
            "\n",
            "Epoch 27/29\n",
            "----------\n",
            "train Loss: 0.02876 Acc: 99.12364 macro-f1: 0.97880\n",
            "vaild Loss: 0.05552 Acc: 98.60926 macro-f1: 0.97475\n",
            "Accumulate epochs training time : 79m 59s\n",
            "\n",
            "\n",
            "Epoch 28/29\n",
            "----------\n",
            "train Loss: 0.02261 Acc: 99.39036 macro-f1: 0.98677\n",
            "vaild Loss: 0.04695 Acc: 99.00934 macro-f1: 0.98258\n",
            "Accumulate epochs training time : 82m 50s\n",
            "\n",
            "\n",
            "Epoch 29/29\n",
            "----------\n",
            "train Loss: 0.01730 Acc: 99.35226 macro-f1: 0.98568\n",
            "vaild Loss: 0.04766 Acc: 98.66641 macro-f1: 0.97988\n",
            "Accumulate epochs training time : 85m 41s\n",
            "\n",
            "\n",
            "one fold - Training complete in 85m 42s\n",
            "2 Best valid loss & Acc: 28 - 99.0\n",
            "1-Fold model saved\n",
            "Total training complete in 172m 32s\n"
          ]
        }
      ],
      "source": [
        "# 참고 블로그 : https://keep-steady.tistory.com/35\n",
        "import gc\n",
        "#             torch.save({'epoch':epoch,\n",
        "#                         'state_dict':model.state_dict(),\n",
        "#                         'optimizer': optimizer.state_dict(),\n",
        "#                         'best_model_wts':best_model_wts,\n",
        "#                         'best_idx':best_idx,\n",
        "#                         'best_acc':best_acc,\n",
        "#                         'best_loss':best_loss,\n",
        "#                         'train_loss' : train_loss,\n",
        "#                         'train_acc' : train_acc,\n",
        "#                         'valid_loss' : valid_loss,\n",
        "#                         'valid_acc' : valid_acc,\n",
        "#                         'total_f1' : total_f1,},\n",
        "#                         #'scaler': scaler.state_dict(),\n",
        "#                         }, f'/content/drive/MyDrive/DACON_이상치 탐지 알고리즘 경진대회/open/{model_version}_model_{K_idx}fold.pth') \n",
        "            \n",
        "def re_train_model(model, criterion, optimizer, scheduler, num_epochs, K_idx): # 중간에 끊겼을때 실행하는 함수\n",
        "    ## 중간 저장된 모델 불러오기\n",
        "    PATH = f'/content/drive/MyDrive/dacon/dacon/model/{model_version}_model_{K_idx}fold.pth'\n",
        "    checkpoint = torch.load(PATH)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    re_epoch = checkpoint['epoch']\n",
        "    best_model_wts = checkpoint['best_model_wts']\n",
        "    best_idx = checkpoint['best_idx']\n",
        "    best_acc = checkpoint['best_acc']\n",
        "    best_loss = checkpoint['best_loss']\n",
        "    train_loss = checkpoint['train_loss']\n",
        "    train_acc = checkpoint['train_acc']\n",
        "    valid_loss = checkpoint['valid_loss']\n",
        "    valid_acc = checkpoint['valid_acc']\n",
        "    total_f1 = checkpoint['total_f1']\n",
        "\n",
        "    ##한 폴드 별 모델 - 가장 좋은 모델 저장\n",
        "    print(f'### restart KFold {K_idx} / {n_splits-1} ###')\n",
        "    one_model_since = time.time() # 하나 모델(폴드)당 시간\n",
        "\n",
        "    for epoch in range(re_epoch, num_epochs):\n",
        "        #one_epoch_since = time.time() ### 한 epoch당 시간\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs- 1))\n",
        "        print('-' * 10)\n",
        "        train_pred=[] # 이거 추가 : f1 스코어 계산\n",
        "        train_y=[] # 이거 추가\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'vaild']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss, running_corrects, num_cnt = 0.0, 0, 0\n",
        "            \n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device) # device = torch.device('cuda')\n",
        "                labels = labels.to(device) # device = torch.device('cuda')\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # # backward + optimize only if in training phase\n",
        "                    # if phase == 'train':\n",
        "                    #     loss.backward()\n",
        "                    #     optimizer.step()\n",
        "\n",
        "                    # backward + optimize only if in training phase + sam\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.first_step(zero_grad=True)\n",
        "                        criterion(model(inputs), labels).backward()\n",
        "                        optimizer.second_step(zero_grad=True)\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                num_cnt += len(labels)\n",
        "\n",
        "                train_pred += preds.detach().cpu().numpy().tolist() #이거 추가\n",
        "                train_y += labels.detach().cpu().data.numpy().tolist() #이거 추가\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "            \n",
        "            epoch_loss = float(running_loss / num_cnt)\n",
        "            epoch_acc  = float((running_corrects.double() / num_cnt).cpu()*100)\n",
        "            f1 = score_function(train_y,  train_pred) # score_function or f1_score\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "                train_acc.append(epoch_acc)\n",
        "            else:\n",
        "                valid_loss.append(epoch_loss)\n",
        "                valid_acc.append(epoch_acc)\n",
        "                total_f1.append(f1)\n",
        "\n",
        "            print('{} Loss: {:.5f} Acc: {:.5f} macro-f1: {:.5f}'.format(phase, epoch_loss, epoch_acc, f1))\n",
        "          \n",
        "            # deep copy the model(최적모델 저장)\n",
        "            if phase == 'vaild' and epoch_acc > best_acc:\n",
        "                best_idx = epoch\n",
        "                best_acc = epoch_acc\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # 20에포크 단위별 중간 저장 : 체크포인트\n",
        "        if (epoch+1)%10 == 0 :\n",
        "            torch.save({'epoch':epoch,\n",
        "                        'state_dict': model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'best_model_wts':best_model_wts,\n",
        "                        'best_idx':best_idx,\n",
        "                        'best_acc':best_acc,\n",
        "                        'best_loss':best_loss,\n",
        "                        'train_loss' : train_loss,\n",
        "                        'train_acc' : train_acc,\n",
        "                        'valid_loss' : valid_loss,\n",
        "                        'valid_acc' : valid_acc,\n",
        "                        'total_f1' : total_f1,},\n",
        "                        #'scaler': scaler.state_dict(),\n",
        "                         f'/content/drive/MyDrive/dacon/dacon/model/{model_version}_model_{K_idx}fold.pth') \n",
        "\n",
        "            print('==> best model saved - %d / %.1f'%(best_idx, best_acc))\n",
        "\n",
        "        # 한 에포크마다 실행 시간 출력하기(누적 시간으로 출력됨)\n",
        "        time_elapsed = time.time() - one_model_since # 한 에포트당 시간 - 누적\n",
        "        print('Accumulate epochs training time : {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        print('\\n')\n",
        "\n",
        "        # 한 에포크마다 필요없는 메모리 지우기 : 지우기 효과는 아직 확인 못해봄\n",
        "        try:      \n",
        "          gc.collect() # cpu 비움\n",
        "          torch.cuda.empty_cache() # gpu 비움\n",
        "        except:\n",
        "          pass\n",
        "          \n",
        "    ## 학습 마무리 후\n",
        "    fold_time_elapsed = time.time() - one_model_since # 한 모델(폴드)당 시간\n",
        "    print('one fold - Training complete in {:.0f}m {:.0f}s'.format(fold_time_elapsed // 60, fold_time_elapsed % 60))\n",
        "\n",
        "    print(f'{K_idx+1} Best valid loss & Acc: %d - %.1f' %(best_idx, best_acc))\n",
        "    # load best model weights\n",
        "    '''\n",
        "    한 에포크 마다 저장된 최적의 모델 가중치로 조정 후 다음 에포크가 돌아감\n",
        "    '''\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    torch.save({'state_dict':model.state_dict(),\n",
        "                'train_loss' : train_loss,\n",
        "                'train_acc' : train_acc,\n",
        "                'valid_loss' : valid_loss,\n",
        "                'valid_acc' : valid_acc,\n",
        "                'total_f1' : total_f1,},\n",
        "                f'/content/drive/MyDrive/dacon/dacon/model/{model_version}_final_best_model_{K_idx}fold.pth') \n",
        "    print(f'{K_idx}-Fold model saved')\n",
        "    return model, best_idx, best_loss, best_acc, train_loss, train_acc, valid_loss, valid_acc, total_f1\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs, K_idx):\n",
        "    ##한 폴드 별 모델 - 가장 좋은 모델 저장\n",
        "    print(f'### KFold {K_idx} / {n_splits-1} ###')\n",
        "    one_model_since = time.time() # 하나 모델(폴드)당 시간\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    train_loss, train_acc, valid_loss, valid_acc, total_f1 = [], [], [], [], []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        #one_epoch_since = time.time() ### 한 epoch당 시간\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs- 1))\n",
        "        print('-' * 10)\n",
        "        train_pred=[] # 이거 추가 : f1 스코어 계산\n",
        "        train_y=[] # 이거 추가\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'vaild']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss, running_corrects, num_cnt = 0.0, 0, 0\n",
        "            \n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device) # device = torch.device('cuda')\n",
        "                labels = labels.to(device) # device = torch.device('cuda')\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # # backward + optimize only if in training phase\n",
        "                    # if phase == 'train':\n",
        "                    #     loss.backward()\n",
        "                    #     optimizer.step()\n",
        "\n",
        "                    # backward + optimize only if in training phase + sam\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.first_step(zero_grad=True)\n",
        "                        criterion(model(inputs), labels).backward()\n",
        "                        optimizer.second_step(zero_grad=True)\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                num_cnt += len(labels)\n",
        "\n",
        "                train_pred += preds.detach().cpu().numpy().tolist() #이거 추가\n",
        "                train_y += labels.detach().cpu().data.numpy().tolist() #이거 추가\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "            \n",
        "            epoch_loss = float(running_loss / num_cnt)\n",
        "            epoch_acc  = float((running_corrects.double() / num_cnt).cpu()*100)\n",
        "            f1 = score_function(train_y,  train_pred) # score_function or f1_score\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "                train_acc.append(epoch_acc)\n",
        "            else:\n",
        "                valid_loss.append(epoch_loss)\n",
        "                valid_acc.append(epoch_acc)\n",
        "                total_f1.append(f1)\n",
        "\n",
        "            print('{} Loss: {:.5f} Acc: {:.5f} macro-f1: {:.5f}'.format(phase, epoch_loss, epoch_acc, f1))\n",
        "          \n",
        "            # deep copy the model(최적모델 저장)\n",
        "            if phase == 'vaild' and epoch_acc > best_acc:\n",
        "                best_idx = epoch\n",
        "                best_acc = epoch_acc\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # 20에포크 단위별 중간 저장 : 체크포인트\n",
        "        if (epoch+1)%20 == 0 :\n",
        "            torch.save({'epoch':epoch,\n",
        "                        'state_dict':model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'best_model_wts':best_model_wts,\n",
        "                        'best_idx':best_idx,\n",
        "                        'best_acc':best_acc,\n",
        "                        'best_loss':best_loss,\n",
        "                        'train_loss' : train_loss,\n",
        "                        'train_acc' : train_acc,\n",
        "                        'valid_loss' : valid_loss,\n",
        "                        'valid_acc' : valid_acc,\n",
        "                        'total_f1' : total_f1,},\n",
        "                        #'scaler': scaler.state_dict(),\n",
        "                         f'/content/drive/MyDrive/dacon/dacon/model/{model_version}_model_{K_idx}fold.pth') \n",
        "\n",
        "            print('==> best model saved - %d / %.1f'%(best_idx, best_acc))\n",
        "\n",
        "        # 한 에포크마다 실행 시간 출력하기(누적 시간으로 출력됨)\n",
        "        time_elapsed = time.time() - one_model_since # 한 에포트당 시간 - 누적\n",
        "        print('Accumulate epochs training time : {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        print('\\n')\n",
        "\n",
        "        # 한 에포크마다 필요없는 메모리 지우기 : 지우기 효과는 아직 확인 못해봄\n",
        "        try:      \n",
        "          gc.collect() # cpu 비움\n",
        "          torch.cuda.empty_cache() # gpu 비움\n",
        "        except:\n",
        "          pass\n",
        "          \n",
        "    ## 학습 마무리 후\n",
        "    fold_time_elapsed = time.time() - one_model_since # 한 모델(폴드)당 시간\n",
        "    print('one fold - Training complete in {:.0f}m {:.0f}s'.format(fold_time_elapsed // 60, fold_time_elapsed % 60))\n",
        "\n",
        "    print(f'{K_idx+1} Best valid loss & Acc: %d - %.1f' %(best_idx, best_acc))\n",
        "    # load best model weights\n",
        "    '''\n",
        "    한 에포크 마다 저장된 최적의 모델 가중치로 조정 후 다음 에포크가 돌아감\n",
        "    '''\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    torch.save({'state_dict':model.state_dict(),\n",
        "                'train_loss' : train_loss,\n",
        "                'train_acc' : train_acc,\n",
        "                'valid_loss' : valid_loss,\n",
        "                'valid_acc' : valid_acc,\n",
        "                'total_f1' : total_f1,},\n",
        "                f'/content/drive/MyDrive/dacon/dacon/model/{model_version}_final_best_model_{K_idx}fold.pth') \n",
        "    print(f'{K_idx}-Fold model saved')\n",
        "    return model, best_idx, best_loss, best_acc, train_loss, train_acc, valid_loss, valid_acc, total_f1\n",
        "\n",
        "    '''\n",
        "    best_idx : 가장 결과 좋은 에포크\n",
        "    best_loss : 가장 좋은 loss\n",
        "    best_acc : 가장 좋은 모델의 정확도\n",
        "    train_loss : 전체 train_loss 리스트\n",
        "    train_acc : 전체 train_acc 리스트\n",
        "    valid_loss : 전체 valid_loss 리스트\n",
        "    valid_acc : 전체 valid_acc 리스트\n",
        "    total_f1 : 전체 f1 리스트\n",
        "    '''\n",
        "\n",
        "# 설정\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # set gpu\n",
        "# model = Network().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# ## 변경된 옵티마이저\n",
        "# base_optimizer = torch.optim.SGD\n",
        "# optimizer_ft = SAM(model.parameters(), base_optimizer, lr=1e-6, momentum=0.9) # momentum?\n",
        "# '''\n",
        "# lr : optimizer_ft > 최저점 설정\n",
        "# '''\n",
        "\n",
        "# lr_scheduler = CosineAnnealingWarmUpRestarts(optimizer_ft, T_0=20, T_mult=1, eta_max=0.1,  T_up=4, gamma=0.5)\n",
        "'''\n",
        "T_0 : 전체 주기\n",
        "T_mult : T_0에 곱할 주기\n",
        "eta_max : 시작점\n",
        "T_up : 다음 최고점까지 올라갈 주기\n",
        "gamma : 다음 주기 때 올라가는 값 계산\n",
        "'''\n",
        "num_epochs = 30\n",
        "\n",
        "# # 사전학습된 가중치와 모델을 가져와 데이터셋으로 추가 모델 학습\n",
        "# train_fold(n_splits, random_seed, shuffle = True, train_imgs, train_labels, criterion, optimizer_ft, lr_scheduler, num_epochs)\n",
        "\n",
        "#def train_fold(n_splits, random_seed, train_imgs, train_labels, shuffle = True, criterion, optimizer_ft, lr_scheduler, num_epochs):\n",
        "\n",
        "n_splits = 2\n",
        "Skfold = StratifiedKFold(n_splits = n_splits, random_state = random_seed, shuffle=True)\n",
        "pred_ensemble = []\n",
        "total_since = time.time() # 전체 실행시간\n",
        "\n",
        "### 폴드 시작 ###\n",
        "start_fold =  int(input()) ## 시작할 폴드 설정, 0부터 카운트\n",
        "restart_state = input('y / n 입력하시오 :') # 학습 중단 될 경우 y\n",
        "for K_idx, (train_idx, val_idx) in enumerate(Skfold.split(np.array(train_imgs), np.array(train_labels))):\n",
        "  \n",
        "  if K_idx >= start_fold: # start_fold = 0일 경우 모든 폴드 실행, start_fold =3 일 경우 3, 4, 5 폴드 실행\n",
        "    print(f\"----------{K_idx} fold start!----------\")\n",
        "    train_img  , val_img   = Subset(train_imgs, train_idx),  Subset(train_imgs, val_idx)\n",
        "    train_label, val_label = Subset(train_labels, train_idx),  Subset(train_labels, val_idx)\n",
        "\n",
        "    dataloaders = {}\n",
        "\n",
        "    # Train\n",
        "    train_dataset = Custom_dataset_Albumentations(img_list = train_img,\n",
        "                                                  labels     = train_label,\n",
        "                                                  transform  = albumentations_transform,\n",
        "                                                  mode = 'train')\n",
        "    dataloaders['train'] = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "    # Val\n",
        "    val_dataset = Custom_dataset_Albumentations(img_list = val_img,\n",
        "                                                labels     = val_label,\n",
        "                                                transform  = albumentations_transform,\n",
        "                                                mode = 'train')\n",
        "    dataloaders['vaild'] = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    best_kfold = 0\n",
        "    best_f1 = 0\n",
        "\n",
        "    model = Network().to(device)\n",
        "    base_optimizer = torch.optim.SGD\n",
        "    optimizer_ft = SAM(model.parameters(), base_optimizer, lr=1e-6, momentum=0.9) # momentum?\n",
        "    lr_scheduler = CosineAnnealingWarmUpRestarts(optimizer_ft, T_0=20, T_mult=1, eta_max=0.1,  T_up=4, gamma=0.5)\n",
        "\n",
        "    # early_stopping = 0\n",
        "    if (K_idx == start_fold) & (restart_state == 'y') : # 중간에 끊긴 경우\n",
        "      model, best_idx, best_loss, best_acc, train_loss, train_acc, valid_loss, valid_acc, total_f1  = re_train_model(model, criterion, optimizer_ft, lr_scheduler, num_epochs, K_idx)\n",
        "    else:\n",
        "      model, best_idx, best_loss, best_acc, train_loss, train_acc, valid_loss, valid_acc, total_f1  = train_model(model, criterion, optimizer_ft, lr_scheduler, num_epochs, K_idx)\n",
        "\n",
        "\n",
        "    fold_time_elapsed = time.time() - total_since # 전체누적시간\n",
        "    #print('one fold - Training complete in {:.0f}m {:.0f}s'.format(fold_time_elapsed // 60, fold_time_elapsed % 60))\n",
        "\n",
        "    if K_idx+1 != n_splits:\n",
        "      print('Accumulate training complete in {:.0f}m {:.0f}s'.format(fold_time_elapsed // 60, fold_time_elapsed % 60))\n",
        "    else:\n",
        "      print('Total training complete in {:.0f}m {:.0f}s'.format(fold_time_elapsed // 60, fold_time_elapsed % 60))\n",
        "\n",
        "  else:\n",
        "    print(f'start_fold : {K_idx} -fold pass')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Qu2QcTdHiUZ9"
      },
      "outputs": [],
      "source": [
        "class Custom_dataset_test(Dataset):\n",
        "    def __init__(self, img_paths, labels, mode='train'):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.mode=mode\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode=='test':\n",
        "          img = self.img_paths[idx]\n",
        "          img = transforms.ToTensor()(img)\n",
        "          img = transforms.Normalize(mean=[0.433, 0.405, 0.396], std=[0.183, 0.177, 0.166])(img)\n",
        "          label = self.labels[idx]\n",
        "          return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKW8H00ubopj"
      },
      "outputs": [],
      "source": [
        "# ## 기본 모델 및 옵티마이저\n",
        "# model = Network().to(device)\n",
        "# base_optimizer = torch.optim.SGD\n",
        "# optimizer_ft = SAM(model.parameters(), base_optimizer, lr=1e-6, momentum=0.9) # momentum?\n",
        "\n",
        "\n",
        "# model_version = 'efficientnetv2_rw_s'\n",
        "# K_idx = '0'\n",
        "\n",
        "# PATH1 =  f'/content/drive/MyDrive/dacon/dacon/model/{model_version}_model_{K_idx}fold.pth'\n",
        "# PATH2 =  f'/content/drive/MyDrive/dacon/dacon/model/{model_version}_final_best_model_{K_idx}fold.pth'\n",
        "\n",
        "# checkpoint = torch.load(PATH1)\n",
        "# model.load_state_dict(checkpoint['best_model_wts'])\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFWoGycRhlzP",
        "outputId": "2703eef5-4943-4642-ae8f-dcc8b793de2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2154/2154 [02:26<00:00, 14.73it/s]\n"
          ]
        }
      ],
      "source": [
        "## 모델을 평가 모드로 변경하고 test 데이터 분류\n",
        "test_png = sorted(glob('/content/drive/MyDrive/dacon/dacon/open/test/*.png'))\n",
        "test_imgs = [img_load(n) for n in tqdm(test_png)]\n",
        "test_dataset = Custom_dataset_test(np.array(test_imgs), np.array([\"tmp\"]*len(test_imgs)), mode='test')\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpL0HYGLe1oT",
        "outputId": "a8712bdd-5abd-498c-f3ad-1fb7ab7a239f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "pred_ensemble = [] # 모든 폴드 결과값\n",
        "\n",
        "for i in range(2): # 0폴드 망함...\n",
        "  pred_prob = [] # 각 폴드별 결과값\n",
        "  model_test = Network(mode = 'test').to(device)\n",
        "  path = f'/content/drive/MyDrive/dacon/dacon/model/{model_version}_model_{i}fold.pth'\n",
        "  checkpoint = torch.load(path)\n",
        "  model_test.load_state_dict(checkpoint['best_model_wts'])\n",
        "  model_test.eval()\n",
        "  with torch.no_grad():\n",
        "      for batch in (test_loader):\n",
        "          x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
        "          with torch.cuda.amp.autocast():\n",
        "              pred = model_test(x)\n",
        "              pred_prob.extend(pred.detach().cpu().numpy())\n",
        "      pred_ensemble.append(pred_prob)\n",
        "\n",
        "print(len(pred_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "SvnqX_2ne1lr",
        "outputId": "9ba00edc-fb08-4b56-8401-6f126a651b13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      index                label\n",
              "0         0      tile-glue_strip\n",
              "1         1            grid-glue\n",
              "2         2  transistor-cut_lead\n",
              "3         3     tile-gray_stroke\n",
              "4         4            tile-good\n",
              "...     ...                  ...\n",
              "2149   2149     tile-gray_stroke\n",
              "2150   2150           screw-good\n",
              "2151   2151            grid-good\n",
              "2152   2152     cable-cable_swap\n",
              "2153   2153          zipper-good\n",
              "\n",
              "[2154 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51b66e88-0609-48ed-8653-f06b5351d3af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tile-glue_strip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>grid-glue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>transistor-cut_lead</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>tile-gray_stroke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>tile-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2149</th>\n",
              "      <td>2149</td>\n",
              "      <td>tile-gray_stroke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2150</th>\n",
              "      <td>2150</td>\n",
              "      <td>screw-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2151</th>\n",
              "      <td>2151</td>\n",
              "      <td>grid-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2152</th>\n",
              "      <td>2152</td>\n",
              "      <td>cable-cable_swap</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2153</th>\n",
              "      <td>2153</td>\n",
              "      <td>zipper-good</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2154 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51b66e88-0609-48ed-8653-f06b5351d3af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51b66e88-0609-48ed-8653-f06b5351d3af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51b66e88-0609-48ed-8653-f06b5351d3af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pred = (np.array(pred_ensemble[0])+ np.array(pred_ensemble[1]))/2\n",
        "f_pred = np.array(pred).argmax(1).tolist()\n",
        "\n",
        "## 숫자로된 레이블을 문자열 레이블로 변경\n",
        "label_decoder = {value:key for key, value in label_unique.items()}\n",
        "f_result = [label_decoder[result] for result in f_pred]\n",
        "\n",
        "from datetime import datetime as dt \n",
        "today = dt.today().strftime('%Y-%m-%d')\n",
        "version = f'efficientnetv2_sam_edit2_data_ver2_transform_{today}'\n",
        "submission = pd.read_csv(f\"/content/drive/MyDrive/dacon/dacon/open/sample_submission.csv\")\n",
        "\n",
        "## 시각적 확인을 위해 문자열 레이블로 이루어진 된 label 필드 생성\n",
        "submission[\"label\"] = f_result\n",
        "display(submission)\n",
        "submission.to_csv(f\"/content/drive/MyDrive/dacon/dacon/open/{version}.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V8CnxMUe1i0",
        "outputId": "8078ca4e-f9ee-4a0d-9f4d-bb83ff81c532"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2154"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = (np.array(pred_ensemble[0])+\n",
        "     np.array(pred_ensemble[1])+\n",
        "     np.array(pred_ensemble[2])+\n",
        "     np.array(pred_ensemble[3])+\n",
        "     np.array(pred_ensemble[4]))/5\n",
        "len(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoPSuVEHe1gw",
        "outputId": "f92be817-c479-4d72-8783-0d39fe46581a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2154"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(np.array(pred_ensemble[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok_8B7Mfe1ea"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkGbFmVVe1VU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kJPLUuEU1pH"
      },
      "outputs": [],
      "source": [
        "1## 결과 그래프 그리기\n",
        "print('best model : %d - %1.f / %.1f'%(best_idx, valid_acc[best_idx], valid_loss[best_idx]))\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.plot(train_acc, 'b-')\n",
        "ax1.plot(valid_acc, 'r-')\n",
        "plt.plot(best_idx, valid_acc[best_idx], 'ro')\n",
        "ax1.set_xlabel('epoch')\n",
        "# Make the y-axis label, ticks and tick labels match the line color.\n",
        "ax1.set_ylabel('acc', color='k')\n",
        "ax1.tick_params('y', colors='k')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(train_loss, 'g-')\n",
        "ax2.plot(valid_loss, 'k-')\n",
        "plt.plot(best_idx, valid_loss[best_idx], 'ro')\n",
        "ax2.set_ylabel('loss', color='k')\n",
        "ax2.tick_params('y', colors='k')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQR1RKgTpKk2"
      },
      "source": [
        "## 모델 추론"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj7gTQv6U11P"
      },
      "outputs": [],
      "source": [
        "## 모델을 평가 모드로 변경하고 test 데이터 분류\n",
        "test_png = sorted(glob('/content/drive/MyDrive/dacon/dacon/open/test/*.png'))\n",
        "test_imgs = [img_load(n) for n in tqdm(test_png)]\n",
        "test_dataset = Custom_dataset_3(np.array(test_imgs), np.array([\"tmp\"]*len(test_imgs)), mode='test')\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "model.eval()\n",
        "f_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in (test_loader):\n",
        "        x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(x)\n",
        "        f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30_rcMw4U13G"
      },
      "outputs": [],
      "source": [
        "## 숫자로된 레이블을 문자열 레이블로 변경\n",
        "label_decoder = {value:key for key, value in label_unique.items()}\n",
        "f_result = [label_decoder[result] for result in f_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zWu-D7WdRmQ"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime as dt \n",
        "today = dt.today().strftime('%Y-%m-%d')\n",
        "version = f'efficientNet_b4_sam_edit2_data_ver2_{today}'\n",
        "submission = pd.read_csv(f\"/content/drive/MyDrive/dacon/dacon/open/sample_submission.csv\")\n",
        "\n",
        "## 시각적 확인을 위해 문자열 레이블로 이루어진 된 label 필드 생성\n",
        "submission[\"label\"] = f_result\n",
        "display(submission)\n",
        "submission.to_csv(f\"/content/drive/MyDrive/dacon/dacon/open/0510{version}.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG_tNWs2zxh8"
      },
      "outputs": [],
      "source": [
        "# # 모델 저장\n",
        "# model_version = f'efficientNet_b4_sam_edit2_data_ver2_{today}'\n",
        "# torch.save(model.state_dict(), f'/content/drive/MyDrive/DACON_이상치 탐지 알고리즘 경진대회/open/{model_version}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7fjAkYDOYse"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "EfficientNetV2_rws_final_kfold_transform_sizeup_newnormal0512.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}