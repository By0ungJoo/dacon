{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "efficientnetv2_rw_s_withsam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP2YAfyr3UAD2xX8xltkKn9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/By0ungJoo/dacon/blob/main/efficientnetv2_rw_s_withsam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpSM_lFOpTEt",
        "outputId": "e9a125b5-c022-4665-8848-c0769eafa688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_v2s_ra2_288-a6477665.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2s_ra2_288-a6477665.pth\n"
          ]
        }
      ],
      "source": [
        "!pip install timm\n",
        "import timm\n",
        "model = timm.create_model('efficientnetv2_rw_s', pretrained=True, num_classes=88)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSj1s9Ogpa0C",
        "outputId": "a834bdc9-38e0-4210-8640-97172c24959d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "#import timm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "t0S8Nf-zpf9E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 기본 설정\n",
        "device = torch.device('cuda')\n",
        "batch_size  = 32\n",
        "random_seed = 1234\n",
        "img_size = 224\n",
        "random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UX9uXZVpn0j",
        "outputId": "cb696858-44b5-4d72-8306-01c03edf0ec3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f374bc12bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def img_load(path):\n",
        "    img = cv2.imread(path)[:,:,::-1] # Return type:\tnumpy.ndarray / 기본적으로 BGR로 불러옴, RGB 변환함\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    return img\n",
        "\n",
        "class Custom_dataset_3(Dataset): # 기본이미지 + 변형이미지 (train 시 2배 증량됨)\n",
        "    def __init__(self, img_paths, labels, mode='train'):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.mode=mode\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode=='train':\n",
        "          if idx < 4277: # 변경 없는 이미지\n",
        "            img = self.img_paths[idx]\n",
        "            img = transforms.ToTensor()(img) # PIL or ndarray -> tensor\n",
        "            img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
        "            label = self.labels[idx]\n",
        "            return img, label\n",
        "\n",
        "          else : # 랜덤 변경된 이미지 추가됨 \n",
        "            img = self.img_paths[idx]\n",
        "            img = Image.fromarray(img)\n",
        "            #img = transforms.Resize((img_size, img_size))(img) # 처음 이미지 불러올때 resize 했으므로 생략\n",
        "            #img = transforms.RandomCrop(img_size)(img)\n",
        "            img = transforms.RandomRotation(90, expand=False)(img)\n",
        "            img = transforms.RandomVerticalFlip()(img)\n",
        "            img = transforms.RandomHorizontalFlip()(img)\n",
        "            img = transforms.ToTensor()(img) # PIL or ndarray -> tensor\n",
        "            img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
        "            label = self.labels[idx]# (원핫인코딩 안할거면 해당 코드만 필요)\n",
        "            return img, label\n",
        "\n",
        "        if self.mode=='test':\n",
        "          img = self.img_paths[idx]\n",
        "          img = transforms.ToTensor()(img)\n",
        "          label = self.labels[idx]# (원핫인코딩 안할거면 해당 코드만 필요)\n",
        "          img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
        "          return img, label"
      ],
      "metadata": {
        "id": "nVETKybspnxs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                e_w = p.grad * scale.to(p)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "                self.state[p][\"e_w\"] = e_w\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        p.grad.norm(p=2).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm"
      ],
      "metadata": {
        "id": "fmx3BEBf2mCU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 이미지 로드 및 전처리\n",
        "# 이미지 경로\n",
        "train_png = sorted(glob('/content/drive/MyDrive/dacon/dacon/open/train1/*.png'))\n",
        "train_imgs = [img_load(m) for m in tqdm(train_png)]\n",
        "\n",
        "#train_imgs = np.load('/content/drive/MyDrive/DACON_이상치 탐지 알고리즘 경진대회/train_imgs_224.npy')\n",
        "train_y = pd.read_csv('/content/drive/MyDrive/dacon/dacon/open/train_df.csv')\n",
        "\n",
        "train_labels = train_y[\"label\"] # 레이블순서는 이미지 파일 순서대로임\n",
        "\n",
        "label_unique = sorted(np.unique(train_labels))\n",
        "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))} # 오름차순으로 레이블별로 숫자 부여(0부터 시작)\n",
        "\n",
        "train_labels = [label_unique[k] for k in train_labels]\n",
        "\n",
        "## 이미지 증량 시만 사용\n",
        "train_imgs = train_imgs + train_imgs\n",
        "train_labels = train_labels+train_labels\n",
        "\n",
        "train_dataset = Custom_dataset_3(np.array(train_imgs), np.array(train_labels), mode='train')\n",
        "\n",
        "### 데이터셋 분리 > 층화추출 & 테스터 데이터 비율 : 0.3, 시드 : 1234, 셔플 = True(default)\n",
        "train_idx, vaild_idx = train_test_split(list(range(len(train_dataset))), stratify=train_labels, test_size=0.3, random_state=random_seed)\n",
        "\n",
        "datasets = {} # 데이터셋을 담을 딕셔너리\n",
        "datasets['train'] = Subset(train_dataset, train_idx)\n",
        "datasets['vaild']  = Subset(train_dataset, vaild_idx)\n",
        "\n",
        "## data loader 선언\n",
        "dataloaders, batch_num = {}, {}\n",
        "dataloaders['train'] = torch.utils.data.DataLoader(datasets['train'],\n",
        "                                                  batch_size=batch_size, shuffle=True,\n",
        "                                                  num_workers=0)\n",
        "dataloaders['vaild']  = torch.utils.data.DataLoader(datasets['vaild'],\n",
        "                                                    batch_size=batch_size, shuffle=False,\n",
        "                                                    num_workers=0)\n",
        "'''\n",
        "데이터 변형 코드를 추가한 후 아래 오류코드가 나와\n",
        "Caught TypeError in DataLoader worker process 0\n",
        "DataLoader > num_workers 를 4 -> 0 로 낮춤\n",
        "'''\n",
        "\n",
        "batch_num['train'], batch_num['vaild'] = len(dataloaders['train']), len(dataloaders['vaild'])\n",
        "print('batch_size : %d,  train/vaild(데이터셋개수/배치사이즈) : %d / %d' % (batch_size, batch_num['train'],batch_num['vaild']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JzWTfaDpnsg",
        "outputId": "fa5942d8-28df-4264-81bb-5db7e1334dff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4277/4277 [04:26<00:00, 16.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size : 32,  train/vaild(데이터셋개수/배치사이즈) : 188 / 81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "### f1 스코어 함수 \n",
        "def score_function(real, pred): # 라이브러리 > sklearn.metrics \n",
        "    score = f1_score(real, pred, average=\"macro\")\n",
        "    return score\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    #train_loss, train_acc, valid_loss, valid_acc = [], [], [], []\n",
        "    train_loss, train_acc, valid_loss, valid_acc, train_f1, valid_f1 = [], [], [], [], [], []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))# - 1))\n",
        "        print('-' * 10)\n",
        "        train_pred=[]\n",
        "        train_y=[]\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'vaild']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss, running_corrects, num_cnt = 0.0, 0, 0\n",
        "            \n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device) # device = torch.device('cuda')\n",
        "                labels = labels.to(device) # device = torch.device('cuda')\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    # if phase == 'train':\n",
        "                    #     loss.backward()\n",
        "                    #     optimizer.step()\n",
        "                     # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.first_step(zero_grad=True)\n",
        "                        criterion(model(inputs), labels).backward()\n",
        "                        optimizer.second_step(zero_grad=True)\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                num_cnt += len(labels)\n",
        "                train_pred += preds.detach().cpu().numpy().tolist()\n",
        "                train_y += labels.detach().cpu().data.numpy().tolist() \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "            \n",
        "            epoch_loss = float(running_loss / num_cnt)\n",
        "            epoch_acc  = float((running_corrects.double() / num_cnt).cpu()*100)\n",
        "            # f1 = score_function(labels.cpu().data, preds.cpu()) # score_function or f1_score\n",
        "            f1 = score_function(train_y,  train_pred) \n",
        "            \n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "                train_acc.append(epoch_acc)\n",
        "                train_f1.append(f1)\n",
        "            else:\n",
        "                valid_loss.append(epoch_loss)\n",
        "                valid_acc.append(epoch_acc)\n",
        "                valid_f1.append(f1)\n",
        "\n",
        "            print('{} Loss: {:.5f} Acc: {:.5f} macro-f1: {:.5f}'.format(phase, epoch_loss, epoch_acc, f1))\n",
        "           \n",
        "            # deep copy the model(최적모델 저장)\n",
        "            if phase == 'vaild' and epoch_acc > best_acc:\n",
        "                best_idx = epoch\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "#                 best_model_wts = copy.deepcopy(model.module.state_dict())\n",
        "                print('==> best model saved - %d / %.1f'%(best_idx, best_acc))\n",
        "        # 한 에포크마다 실행 시간 출력하기(누적 시간으로 출력됨)\n",
        "        time_elapsed = time.time() - since\n",
        "        print('each epochs training time : {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        print('\\n')\n",
        "\n",
        "        # 한 에포크마다 필요없는 메모리 지우기 : 지우기 효과는 아직 확인 못해봄\n",
        "        try:      \n",
        "          gc.collect() # cpu 비움\n",
        "          torch.cuda.empty_cache() # gpu 비움\n",
        "        except:\n",
        "          pass\n",
        "          \n",
        "    ## 학습 마무리 후\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best valid Acc: %d - %.1f' %(best_idx, best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    '''\n",
        "    한 에포크 마다 저장된 최적의 모델 가중치로 조정 후 다음 에포크가 돌아감\n",
        "    '''\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/dacon/dacon/model/president_model.pt')\n",
        "    print('model saved')\n",
        "    return model, best_idx, best_acc, train_loss, train_acc, valid_loss, valid_acc, train_f1, valid_f1\n",
        "\n",
        "# 설정\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # set gpu\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer_ft = optim.SGD(model.parameters(), \n",
        "#                          lr = 0.05,\n",
        "#                          momentum=0.9,\n",
        "#                          weight_decay=1e-4)\n",
        "\n",
        "base_optimizer = torch.optim.SGD\n",
        "optimizer_ft = SAM(model.parameters(), base_optimizer, lr=0.001, momentum=0.9)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft,\n",
        "                                                step_size = 5,\n",
        "                                                gamma = 0.75)\n",
        "\n",
        "lmbda = lambda epoch: 0.98739\n",
        "exp_lr_scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer_ft, lr_lambda=lmbda)\n",
        "\n",
        "# 사전학습된 가중치와 모델을 가져와 데이터셋으로 추가 모델 학습\n",
        "model, best_idx, best_acc, train_loss, train_acc, valid_loss, valid_acc, train_f1, valid_f1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=100)"
      ],
      "metadata": {
        "id": "mnCGkEC8pnqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1964c08e-bf5d-43b0-a9df-7716ae0e9c99"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/100\n",
            "----------\n",
            "train Loss: 1.97872 Acc: 70.58627 macro-f1: 0.13798\n",
            "vaild Loss: 1.03879 Acc: 84.69030 macro-f1: 0.14373\n",
            "==> best model saved - 0 / 84.7\n",
            "each epochs training time : 1m 52s\n",
            "\n",
            "\n",
            "Epoch 1/100\n",
            "----------\n",
            "train Loss: 0.96250 Acc: 84.76700 macro-f1: 0.15594\n",
            "vaild Loss: 0.87369 Acc: 84.96299 macro-f1: 0.15776\n",
            "==> best model saved - 1 / 85.0\n",
            "each epochs training time : 3m 43s\n",
            "\n",
            "\n",
            "Epoch 2/100\n",
            "----------\n",
            "train Loss: 0.81731 Acc: 84.80040 macro-f1: 0.15607\n",
            "vaild Loss: 0.77400 Acc: 85.15777 macro-f1: 0.16290\n",
            "==> best model saved - 2 / 85.2\n",
            "each epochs training time : 5m 34s\n",
            "\n",
            "\n",
            "Epoch 3/100\n",
            "----------\n",
            "train Loss: 0.71945 Acc: 85.20127 macro-f1: 0.17945\n",
            "vaild Loss: 0.69397 Acc: 85.66420 macro-f1: 0.18820\n",
            "==> best model saved - 3 / 85.7\n",
            "each epochs training time : 7m 25s\n",
            "\n",
            "\n",
            "Epoch 4/100\n",
            "----------\n",
            "train Loss: 0.65170 Acc: 85.78587 macro-f1: 0.21672\n",
            "vaild Loss: 0.63546 Acc: 86.05376 macro-f1: 0.21901\n",
            "==> best model saved - 4 / 86.1\n",
            "each epochs training time : 9m 16s\n",
            "\n",
            "\n",
            "Epoch 5/100\n",
            "----------\n",
            "train Loss: 0.59157 Acc: 86.25355 macro-f1: 0.24116\n",
            "vaild Loss: 0.58722 Acc: 86.56019 macro-f1: 0.24370\n",
            "==> best model saved - 5 / 86.6\n",
            "each epochs training time : 11m 8s\n",
            "\n",
            "\n",
            "Epoch 6/100\n",
            "----------\n",
            "train Loss: 0.53945 Acc: 87.02188 macro-f1: 0.29551\n",
            "vaild Loss: 0.55621 Acc: 86.83288 macro-f1: 0.29173\n",
            "==> best model saved - 6 / 86.8\n",
            "each epochs training time : 12m 59s\n",
            "\n",
            "\n",
            "Epoch 7/100\n",
            "----------\n",
            "train Loss: 0.50125 Acc: 87.43945 macro-f1: 0.31744\n",
            "vaild Loss: 0.52506 Acc: 87.06661 macro-f1: 0.31158\n",
            "==> best model saved - 7 / 87.1\n",
            "each epochs training time : 14m 50s\n",
            "\n",
            "\n",
            "Epoch 8/100\n",
            "----------\n",
            "train Loss: 0.46294 Acc: 88.00735 macro-f1: 0.36134\n",
            "vaild Loss: 0.48713 Acc: 88.00156 macro-f1: 0.35996\n",
            "==> best model saved - 8 / 88.0\n",
            "each epochs training time : 16m 41s\n",
            "\n",
            "\n",
            "Epoch 9/100\n",
            "----------\n",
            "train Loss: 0.42832 Acc: 88.80909 macro-f1: 0.41386\n",
            "vaild Loss: 0.46525 Acc: 88.07947 macro-f1: 0.39868\n",
            "==> best model saved - 9 / 88.1\n",
            "each epochs training time : 18m 32s\n",
            "\n",
            "\n",
            "Epoch 10/100\n",
            "----------\n",
            "train Loss: 0.40885 Acc: 89.31017 macro-f1: 0.45242\n",
            "vaild Loss: 0.44166 Acc: 88.70277 macro-f1: 0.44019\n",
            "==> best model saved - 10 / 88.7\n",
            "each epochs training time : 20m 23s\n",
            "\n",
            "\n",
            "Epoch 11/100\n",
            "----------\n",
            "train Loss: 0.38659 Acc: 89.62753 macro-f1: 0.47143\n",
            "vaild Loss: 0.42471 Acc: 88.85859 macro-f1: 0.45976\n",
            "==> best model saved - 11 / 88.9\n",
            "each epochs training time : 22m 14s\n",
            "\n",
            "\n",
            "Epoch 12/100\n",
            "----------\n",
            "train Loss: 0.36123 Acc: 90.37915 macro-f1: 0.51733\n",
            "vaild Loss: 0.40708 Acc: 89.63771 macro-f1: 0.50449\n",
            "==> best model saved - 12 / 89.6\n",
            "each epochs training time : 24m 6s\n",
            "\n",
            "\n",
            "Epoch 13/100\n",
            "----------\n",
            "train Loss: 0.34699 Acc: 90.83013 macro-f1: 0.54192\n",
            "vaild Loss: 0.39485 Acc: 89.87145 macro-f1: 0.52298\n",
            "==> best model saved - 13 / 89.9\n",
            "each epochs training time : 25m 57s\n",
            "\n",
            "\n",
            "Epoch 14/100\n",
            "----------\n",
            "train Loss: 0.32269 Acc: 91.29781 macro-f1: 0.57418\n",
            "vaild Loss: 0.37668 Acc: 89.83249 macro-f1: 0.54607\n",
            "each epochs training time : 27m 48s\n",
            "\n",
            "\n",
            "Epoch 15/100\n",
            "----------\n",
            "train Loss: 0.31298 Acc: 91.59846 macro-f1: 0.59911\n",
            "vaild Loss: 0.36150 Acc: 89.91040 macro-f1: 0.56494\n",
            "==> best model saved - 15 / 89.9\n",
            "each epochs training time : 29m 38s\n",
            "\n",
            "\n",
            "Epoch 16/100\n",
            "----------\n",
            "train Loss: 0.29553 Acc: 91.79890 macro-f1: 0.60364\n",
            "vaild Loss: 0.34654 Acc: 90.45578 macro-f1: 0.57933\n",
            "==> best model saved - 16 / 90.5\n",
            "each epochs training time : 31m 29s\n",
            "\n",
            "\n",
            "Epoch 17/100\n",
            "----------\n",
            "train Loss: 0.28714 Acc: 92.18306 macro-f1: 0.62835\n",
            "vaild Loss: 0.33952 Acc: 90.68952 macro-f1: 0.60235\n",
            "==> best model saved - 17 / 90.7\n",
            "each epochs training time : 33m 19s\n",
            "\n",
            "\n",
            "Epoch 18/100\n",
            "----------\n",
            "train Loss: 0.26983 Acc: 92.68415 macro-f1: 0.66330\n",
            "vaild Loss: 0.34340 Acc: 90.72848 macro-f1: 0.62811\n",
            "==> best model saved - 18 / 90.7\n",
            "each epochs training time : 35m 10s\n",
            "\n",
            "\n",
            "Epoch 19/100\n",
            "----------\n",
            "train Loss: 0.26676 Acc: 92.88458 macro-f1: 0.67293\n",
            "vaild Loss: 0.33333 Acc: 90.53370 macro-f1: 0.63545\n",
            "each epochs training time : 37m 0s\n",
            "\n",
            "\n",
            "Epoch 20/100\n",
            "----------\n",
            "train Loss: 0.24822 Acc: 93.13513 macro-f1: 0.68854\n",
            "vaild Loss: 0.30623 Acc: 91.11804 macro-f1: 0.65425\n",
            "==> best model saved - 20 / 91.1\n",
            "each epochs training time : 38m 51s\n",
            "\n",
            "\n",
            "Epoch 21/100\n",
            "----------\n",
            "train Loss: 0.24074 Acc: 93.30215 macro-f1: 0.69060\n",
            "vaild Loss: 0.30025 Acc: 91.62446 macro-f1: 0.66229\n",
            "==> best model saved - 21 / 91.6\n",
            "each epochs training time : 40m 41s\n",
            "\n",
            "\n",
            "Epoch 22/100\n",
            "----------\n",
            "train Loss: 0.23699 Acc: 93.56940 macro-f1: 0.71189\n",
            "vaild Loss: 0.33535 Acc: 90.68952 macro-f1: 0.66633\n",
            "each epochs training time : 42m 31s\n",
            "\n",
            "\n",
            "Epoch 23/100\n",
            "----------\n",
            "train Loss: 0.22788 Acc: 93.95357 macro-f1: 0.73877\n",
            "vaild Loss: 0.28422 Acc: 91.62446 macro-f1: 0.69793\n",
            "each epochs training time : 44m 21s\n",
            "\n",
            "\n",
            "Epoch 24/100\n",
            "----------\n",
            "train Loss: 0.21645 Acc: 94.10389 macro-f1: 0.74290\n",
            "vaild Loss: 0.28390 Acc: 91.70238 macro-f1: 0.70501\n",
            "==> best model saved - 24 / 91.7\n",
            "each epochs training time : 46m 12s\n",
            "\n",
            "\n",
            "Epoch 25/100\n",
            "----------\n",
            "train Loss: 0.21266 Acc: 94.22081 macro-f1: 0.75188\n",
            "vaild Loss: 0.28487 Acc: 91.74133 macro-f1: 0.71153\n",
            "==> best model saved - 25 / 91.7\n",
            "each epochs training time : 48m 3s\n",
            "\n",
            "\n",
            "Epoch 26/100\n",
            "----------\n",
            "train Loss: 0.20602 Acc: 94.55487 macro-f1: 0.76673\n",
            "vaild Loss: 0.26439 Acc: 92.63732 macro-f1: 0.73707\n",
            "==> best model saved - 26 / 92.6\n",
            "each epochs training time : 49m 53s\n",
            "\n",
            "\n",
            "Epoch 27/100\n",
            "----------\n",
            "train Loss: 0.19638 Acc: 94.72190 macro-f1: 0.78662\n",
            "vaild Loss: 0.27474 Acc: 91.62446 macro-f1: 0.73603\n",
            "each epochs training time : 51m 43s\n",
            "\n",
            "\n",
            "Epoch 28/100\n",
            "----------\n",
            "train Loss: 0.19757 Acc: 94.60498 macro-f1: 0.77720\n",
            "vaild Loss: 0.25230 Acc: 92.24776 macro-f1: 0.73527\n",
            "each epochs training time : 53m 33s\n",
            "\n",
            "\n",
            "Epoch 29/100\n",
            "----------\n",
            "train Loss: 0.18300 Acc: 94.97244 macro-f1: 0.79949\n",
            "vaild Loss: 0.24669 Acc: 92.91001 macro-f1: 0.76310\n",
            "==> best model saved - 29 / 92.9\n",
            "each epochs training time : 55m 24s\n",
            "\n",
            "\n",
            "Epoch 30/100\n",
            "----------\n",
            "train Loss: 0.17930 Acc: 95.15617 macro-f1: 0.80153\n",
            "vaild Loss: 0.25168 Acc: 92.48150 macro-f1: 0.75938\n",
            "each epochs training time : 57m 14s\n",
            "\n",
            "\n",
            "Epoch 31/100\n",
            "----------\n",
            "train Loss: 0.17594 Acc: 95.03925 macro-f1: 0.80029\n",
            "vaild Loss: 0.24280 Acc: 93.18270 macro-f1: 0.76796\n",
            "==> best model saved - 31 / 93.2\n",
            "each epochs training time : 59m 4s\n",
            "\n",
            "\n",
            "Epoch 32/100\n",
            "----------\n",
            "train Loss: 0.17243 Acc: 95.60715 macro-f1: 0.82678\n",
            "vaild Loss: 0.25023 Acc: 92.44254 macro-f1: 0.78026\n",
            "each epochs training time : 60m 54s\n",
            "\n",
            "\n",
            "Epoch 33/100\n",
            "----------\n",
            "train Loss: 0.17152 Acc: 95.42342 macro-f1: 0.81732\n",
            "vaild Loss: 0.24225 Acc: 92.98792 macro-f1: 0.77953\n",
            "each epochs training time : 62m 44s\n",
            "\n",
            "\n",
            "Epoch 34/100\n",
            "----------\n",
            "train Loss: 0.16134 Acc: 95.89110 macro-f1: 0.84166\n",
            "vaild Loss: 0.24770 Acc: 92.83210 macro-f1: 0.79341\n",
            "each epochs training time : 64m 35s\n",
            "\n",
            "\n",
            "Epoch 35/100\n",
            "----------\n",
            "train Loss: 0.16037 Acc: 95.57374 macro-f1: 0.82696\n",
            "vaild Loss: 0.22408 Acc: 93.29957 macro-f1: 0.78675\n",
            "==> best model saved - 35 / 93.3\n",
            "each epochs training time : 66m 25s\n",
            "\n",
            "\n",
            "Epoch 36/100\n",
            "----------\n",
            "train Loss: 0.16000 Acc: 95.92450 macro-f1: 0.84616\n",
            "vaild Loss: 0.23686 Acc: 93.14375 macro-f1: 0.80451\n",
            "each epochs training time : 68m 15s\n",
            "\n",
            "\n",
            "Epoch 37/100\n",
            "----------\n",
            "train Loss: 0.15198 Acc: 95.89110 macro-f1: 0.83859\n",
            "vaild Loss: 0.22995 Acc: 93.26062 macro-f1: 0.79380\n",
            "each epochs training time : 70m 5s\n",
            "\n",
            "\n",
            "Epoch 38/100\n",
            "----------\n",
            "train Loss: 0.14131 Acc: 96.24186 macro-f1: 0.85718\n",
            "vaild Loss: 0.24057 Acc: 93.02688 macro-f1: 0.81057\n",
            "each epochs training time : 71m 55s\n",
            "\n",
            "\n",
            "Epoch 39/100\n",
            "----------\n",
            "train Loss: 0.14547 Acc: 95.89110 macro-f1: 0.83696\n",
            "vaild Loss: 0.21453 Acc: 93.68913 macro-f1: 0.80032\n",
            "==> best model saved - 39 / 93.7\n",
            "each epochs training time : 73m 45s\n",
            "\n",
            "\n",
            "Epoch 40/100\n",
            "----------\n",
            "train Loss: 0.14010 Acc: 96.22515 macro-f1: 0.85898\n",
            "vaild Loss: 0.21703 Acc: 94.15660 macro-f1: 0.82460\n",
            "==> best model saved - 40 / 94.2\n",
            "each epochs training time : 75m 36s\n",
            "\n",
            "\n",
            "Epoch 41/100\n",
            "----------\n",
            "train Loss: 0.13579 Acc: 96.62602 macro-f1: 0.87162\n",
            "vaild Loss: 0.23317 Acc: 93.33853 macro-f1: 0.82386\n",
            "each epochs training time : 77m 26s\n",
            "\n",
            "\n",
            "Epoch 42/100\n",
            "----------\n",
            "train Loss: 0.13002 Acc: 96.75965 macro-f1: 0.88322\n",
            "vaild Loss: 0.20243 Acc: 94.03974 macro-f1: 0.84083\n",
            "each epochs training time : 79m 16s\n",
            "\n",
            "\n",
            "Epoch 43/100\n",
            "----------\n",
            "train Loss: 0.13354 Acc: 96.22515 macro-f1: 0.85425\n",
            "vaild Loss: 0.20622 Acc: 94.00078 macro-f1: 0.81885\n",
            "each epochs training time : 81m 6s\n",
            "\n",
            "\n",
            "Epoch 44/100\n",
            "----------\n",
            "train Loss: 0.12589 Acc: 96.72624 macro-f1: 0.87794\n",
            "vaild Loss: 0.20067 Acc: 94.11765 macro-f1: 0.83711\n",
            "each epochs training time : 82m 56s\n",
            "\n",
            "\n",
            "Epoch 45/100\n",
            "----------\n",
            "train Loss: 0.12024 Acc: 96.96008 macro-f1: 0.88733\n",
            "vaild Loss: 0.19404 Acc: 94.23451 macro-f1: 0.84597\n",
            "==> best model saved - 45 / 94.2\n",
            "each epochs training time : 84m 46s\n",
            "\n",
            "\n",
            "Epoch 46/100\n",
            "----------\n",
            "train Loss: 0.11706 Acc: 97.01019 macro-f1: 0.89122\n",
            "vaild Loss: 0.17971 Acc: 94.93572 macro-f1: 0.85894\n",
            "==> best model saved - 46 / 94.9\n",
            "each epochs training time : 86m 37s\n",
            "\n",
            "\n",
            "Epoch 47/100\n",
            "----------\n",
            "train Loss: 0.11773 Acc: 96.79305 macro-f1: 0.88036\n",
            "vaild Loss: 0.20933 Acc: 93.80600 macro-f1: 0.83756\n",
            "each epochs training time : 88m 27s\n",
            "\n",
            "\n",
            "Epoch 48/100\n",
            "----------\n",
            "train Loss: 0.11393 Acc: 97.06030 macro-f1: 0.89052\n",
            "vaild Loss: 0.19911 Acc: 94.35138 macro-f1: 0.85543\n",
            "each epochs training time : 90m 17s\n",
            "\n",
            "\n",
            "Epoch 49/100\n",
            "----------\n",
            "train Loss: 0.11619 Acc: 97.07700 macro-f1: 0.88750\n",
            "vaild Loss: 0.18535 Acc: 94.54616 macro-f1: 0.85111\n",
            "each epochs training time : 92m 8s\n",
            "\n",
            "\n",
            "Epoch 50/100\n",
            "----------\n",
            "train Loss: 0.10964 Acc: 97.19392 macro-f1: 0.89644\n",
            "vaild Loss: 0.19405 Acc: 93.92287 macro-f1: 0.84808\n",
            "each epochs training time : 93m 58s\n",
            "\n",
            "\n",
            "Epoch 51/100\n",
            "----------\n",
            "train Loss: 0.10796 Acc: 97.14381 macro-f1: 0.89615\n",
            "vaild Loss: 0.21773 Acc: 93.72809 macro-f1: 0.84940\n",
            "each epochs training time : 95m 49s\n",
            "\n",
            "\n",
            "Epoch 52/100\n",
            "----------\n",
            "train Loss: 0.10609 Acc: 97.06030 macro-f1: 0.89394\n",
            "vaild Loss: 0.21154 Acc: 93.84496 macro-f1: 0.84910\n",
            "each epochs training time : 97m 39s\n",
            "\n",
            "\n",
            "Epoch 53/100\n",
            "----------\n",
            "train Loss: 0.10848 Acc: 97.22733 macro-f1: 0.89849\n",
            "vaild Loss: 0.19281 Acc: 94.19556 macro-f1: 0.85356\n",
            "each epochs training time : 99m 29s\n",
            "\n",
            "\n",
            "Epoch 54/100\n",
            "----------\n",
            "train Loss: 0.10269 Acc: 97.44446 macro-f1: 0.90739\n",
            "vaild Loss: 0.17404 Acc: 94.74094 macro-f1: 0.86551\n",
            "each epochs training time : 101m 20s\n",
            "\n",
            "\n",
            "Epoch 55/100\n",
            "----------\n",
            "train Loss: 0.10181 Acc: 97.54468 macro-f1: 0.91196\n",
            "vaild Loss: 0.19880 Acc: 93.96182 macro-f1: 0.86278\n",
            "each epochs training time : 103m 10s\n",
            "\n",
            "\n",
            "Epoch 56/100\n",
            "----------\n",
            "train Loss: 0.10190 Acc: 97.44446 macro-f1: 0.90899\n",
            "vaild Loss: 0.17127 Acc: 94.97468 macro-f1: 0.87264\n",
            "==> best model saved - 56 / 95.0\n",
            "each epochs training time : 105m 1s\n",
            "\n",
            "\n",
            "Epoch 57/100\n",
            "----------\n",
            "train Loss: 0.09508 Acc: 97.62819 macro-f1: 0.91325\n",
            "vaild Loss: 0.17296 Acc: 94.46825 macro-f1: 0.87054\n",
            "each epochs training time : 106m 52s\n",
            "\n",
            "\n",
            "Epoch 58/100\n",
            "----------\n",
            "train Loss: 0.09478 Acc: 97.76182 macro-f1: 0.91984\n",
            "vaild Loss: 0.17172 Acc: 94.85781 macro-f1: 0.87914\n",
            "each epochs training time : 108m 43s\n",
            "\n",
            "\n",
            "Epoch 59/100\n",
            "----------\n",
            "train Loss: 0.09946 Acc: 97.46117 macro-f1: 0.90844\n",
            "vaild Loss: 0.16822 Acc: 94.66303 macro-f1: 0.86944\n",
            "each epochs training time : 110m 33s\n",
            "\n",
            "\n",
            "Epoch 60/100\n",
            "----------\n",
            "train Loss: 0.09130 Acc: 97.62819 macro-f1: 0.91599\n",
            "vaild Loss: 0.16771 Acc: 94.70199 macro-f1: 0.87272\n",
            "each epochs training time : 112m 24s\n",
            "\n",
            "\n",
            "Epoch 61/100\n",
            "----------\n",
            "train Loss: 0.09491 Acc: 97.49457 macro-f1: 0.91180\n",
            "vaild Loss: 0.16107 Acc: 95.32528 macro-f1: 0.87705\n",
            "==> best model saved - 61 / 95.3\n",
            "each epochs training time : 114m 15s\n",
            "\n",
            "\n",
            "Epoch 62/100\n",
            "----------\n",
            "train Loss: 0.09005 Acc: 97.52798 macro-f1: 0.91243\n",
            "vaild Loss: 0.16990 Acc: 94.89677 macro-f1: 0.87490\n",
            "each epochs training time : 116m 6s\n",
            "\n",
            "\n",
            "Epoch 63/100\n",
            "----------\n",
            "train Loss: 0.08709 Acc: 97.77852 macro-f1: 0.91998\n",
            "vaild Loss: 0.16063 Acc: 95.05259 macro-f1: 0.87935\n",
            "each epochs training time : 117m 57s\n",
            "\n",
            "\n",
            "Epoch 64/100\n",
            "----------\n",
            "train Loss: 0.08891 Acc: 97.62819 macro-f1: 0.91758\n",
            "vaild Loss: 0.15855 Acc: 95.13050 macro-f1: 0.88025\n",
            "each epochs training time : 119m 47s\n",
            "\n",
            "\n",
            "Epoch 65/100\n",
            "----------\n",
            "train Loss: 0.08653 Acc: 97.87874 macro-f1: 0.92553\n",
            "vaild Loss: 0.16827 Acc: 95.20841 macro-f1: 0.89173\n",
            "each epochs training time : 121m 38s\n",
            "\n",
            "\n",
            "Epoch 66/100\n",
            "----------\n",
            "train Loss: 0.08895 Acc: 97.57809 macro-f1: 0.91693\n",
            "vaild Loss: 0.19381 Acc: 94.07869 macro-f1: 0.86707\n",
            "each epochs training time : 123m 29s\n",
            "\n",
            "\n",
            "Epoch 67/100\n",
            "----------\n",
            "train Loss: 0.08167 Acc: 97.79522 macro-f1: 0.91948\n",
            "vaild Loss: 0.16954 Acc: 94.74094 macro-f1: 0.87974\n",
            "each epochs training time : 125m 19s\n",
            "\n",
            "\n",
            "Epoch 68/100\n",
            "----------\n",
            "train Loss: 0.08544 Acc: 97.74511 macro-f1: 0.91695\n",
            "vaild Loss: 0.15528 Acc: 95.59797 macro-f1: 0.88504\n",
            "==> best model saved - 68 / 95.6\n",
            "each epochs training time : 127m 10s\n",
            "\n",
            "\n",
            "Epoch 69/100\n",
            "----------\n",
            "train Loss: 0.08325 Acc: 97.96225 macro-f1: 0.92974\n",
            "vaild Loss: 0.14718 Acc: 95.55902 macro-f1: 0.89343\n",
            "each epochs training time : 129m 1s\n",
            "\n",
            "\n",
            "Epoch 70/100\n",
            "----------\n",
            "train Loss: 0.07863 Acc: 97.94555 macro-f1: 0.92603\n",
            "vaild Loss: 0.14983 Acc: 95.79275 macro-f1: 0.89486\n",
            "==> best model saved - 70 / 95.8\n",
            "each epochs training time : 130m 52s\n",
            "\n",
            "\n",
            "Epoch 71/100\n",
            "----------\n",
            "train Loss: 0.07960 Acc: 97.99566 macro-f1: 0.92521\n",
            "vaild Loss: 0.16201 Acc: 95.13050 macro-f1: 0.88852\n",
            "each epochs training time : 132m 43s\n",
            "\n",
            "\n",
            "Epoch 72/100\n",
            "----------\n",
            "train Loss: 0.08059 Acc: 97.99566 macro-f1: 0.92634\n",
            "vaild Loss: 0.16049 Acc: 95.48111 macro-f1: 0.89337\n",
            "each epochs training time : 134m 34s\n",
            "\n",
            "\n",
            "Epoch 73/100\n",
            "----------\n",
            "train Loss: 0.09051 Acc: 97.61149 macro-f1: 0.91428\n",
            "vaild Loss: 0.18457 Acc: 94.42929 macro-f1: 0.87406\n",
            "each epochs training time : 136m 24s\n",
            "\n",
            "\n",
            "Epoch 74/100\n",
            "----------\n",
            "train Loss: 0.08071 Acc: 97.91214 macro-f1: 0.92319\n",
            "vaild Loss: 0.15577 Acc: 95.63693 macro-f1: 0.89166\n",
            "each epochs training time : 138m 15s\n",
            "\n",
            "\n",
            "Epoch 75/100\n",
            "----------\n",
            "train Loss: 0.07698 Acc: 98.14598 macro-f1: 0.93255\n",
            "vaild Loss: 0.16567 Acc: 94.77990 macro-f1: 0.88740\n",
            "each epochs training time : 140m 6s\n",
            "\n",
            "\n",
            "Epoch 76/100\n",
            "----------\n",
            "train Loss: 0.08316 Acc: 97.86203 macro-f1: 0.92454\n",
            "vaild Loss: 0.15796 Acc: 95.40319 macro-f1: 0.89051\n",
            "each epochs training time : 141m 57s\n",
            "\n",
            "\n",
            "Epoch 77/100\n",
            "----------\n",
            "train Loss: 0.07694 Acc: 98.09587 macro-f1: 0.93416\n",
            "vaild Loss: 0.17078 Acc: 94.42929 macro-f1: 0.88293\n",
            "each epochs training time : 143m 47s\n",
            "\n",
            "\n",
            "Epoch 78/100\n",
            "----------\n",
            "train Loss: 0.07650 Acc: 97.96225 macro-f1: 0.92527\n",
            "vaild Loss: 0.14790 Acc: 95.48111 macro-f1: 0.89234\n",
            "each epochs training time : 145m 38s\n",
            "\n",
            "\n",
            "Epoch 79/100\n",
            "----------\n",
            "train Loss: 0.07876 Acc: 97.97895 macro-f1: 0.92487\n",
            "vaild Loss: 0.15297 Acc: 95.40319 macro-f1: 0.88937\n",
            "each epochs training time : 147m 29s\n",
            "\n",
            "\n",
            "Epoch 80/100\n",
            "----------\n",
            "train Loss: 0.07598 Acc: 97.99566 macro-f1: 0.92947\n",
            "vaild Loss: 0.16022 Acc: 95.09155 macro-f1: 0.89293\n",
            "each epochs training time : 149m 20s\n",
            "\n",
            "\n",
            "Epoch 81/100\n",
            "----------\n",
            "train Loss: 0.07143 Acc: 98.07917 macro-f1: 0.93328\n",
            "vaild Loss: 0.14175 Acc: 95.75380 macro-f1: 0.89813\n",
            "each epochs training time : 151m 10s\n",
            "\n",
            "\n",
            "Epoch 82/100\n",
            "----------\n",
            "train Loss: 0.07370 Acc: 98.09587 macro-f1: 0.93271\n",
            "vaild Loss: 0.14146 Acc: 95.94858 macro-f1: 0.90034\n",
            "==> best model saved - 82 / 95.9\n",
            "each epochs training time : 153m 1s\n",
            "\n",
            "\n",
            "Epoch 83/100\n",
            "----------\n",
            "train Loss: 0.06932 Acc: 98.22950 macro-f1: 0.93637\n",
            "vaild Loss: 0.15692 Acc: 95.52006 macro-f1: 0.90160\n",
            "each epochs training time : 154m 52s\n",
            "\n",
            "\n",
            "Epoch 84/100\n",
            "----------\n",
            "train Loss: 0.06983 Acc: 98.17939 macro-f1: 0.93475\n",
            "vaild Loss: 0.14850 Acc: 95.71484 macro-f1: 0.90333\n",
            "each epochs training time : 156m 43s\n",
            "\n",
            "\n",
            "Epoch 85/100\n",
            "----------\n",
            "train Loss: 0.06813 Acc: 98.29631 macro-f1: 0.93774\n",
            "vaild Loss: 0.13048 Acc: 96.26023 macro-f1: 0.91023\n",
            "==> best model saved - 85 / 96.3\n",
            "each epochs training time : 158m 34s\n",
            "\n",
            "\n",
            "Epoch 86/100\n",
            "----------\n",
            "train Loss: 0.07067 Acc: 98.11258 macro-f1: 0.93399\n",
            "vaild Loss: 0.14040 Acc: 95.59797 macro-f1: 0.89653\n",
            "each epochs training time : 160m 24s\n",
            "\n",
            "\n",
            "Epoch 87/100\n",
            "----------\n",
            "train Loss: 0.06751 Acc: 98.41323 macro-f1: 0.94406\n",
            "vaild Loss: 0.15342 Acc: 95.44215 macro-f1: 0.90548\n",
            "each epochs training time : 162m 15s\n",
            "\n",
            "\n",
            "Epoch 88/100\n",
            "----------\n",
            "train Loss: 0.06762 Acc: 98.37982 macro-f1: 0.94288\n",
            "vaild Loss: 0.14416 Acc: 95.71484 macro-f1: 0.90762\n",
            "each epochs training time : 164m 6s\n",
            "\n",
            "\n",
            "Epoch 89/100\n",
            "----------\n",
            "train Loss: 0.06619 Acc: 98.29631 macro-f1: 0.93803\n",
            "vaild Loss: 0.13089 Acc: 95.90962 macro-f1: 0.90602\n",
            "each epochs training time : 165m 57s\n",
            "\n",
            "\n",
            "Epoch 90/100\n",
            "----------\n",
            "train Loss: 0.06747 Acc: 98.34642 macro-f1: 0.94034\n",
            "vaild Loss: 0.13441 Acc: 96.37709 macro-f1: 0.91220\n",
            "==> best model saved - 90 / 96.4\n",
            "each epochs training time : 167m 48s\n",
            "\n",
            "\n",
            "Epoch 91/100\n",
            "----------\n",
            "train Loss: 0.06495 Acc: 98.32971 macro-f1: 0.93866\n",
            "vaild Loss: 0.13276 Acc: 96.10440 macro-f1: 0.90803\n",
            "each epochs training time : 169m 39s\n",
            "\n",
            "\n",
            "Epoch 92/100\n",
            "----------\n",
            "train Loss: 0.06388 Acc: 98.36312 macro-f1: 0.94070\n",
            "vaild Loss: 0.14337 Acc: 95.55902 macro-f1: 0.90454\n",
            "each epochs training time : 171m 29s\n",
            "\n",
            "\n",
            "Epoch 93/100\n",
            "----------\n",
            "train Loss: 0.06252 Acc: 98.54685 macro-f1: 0.94797\n",
            "vaild Loss: 0.12939 Acc: 96.37709 macro-f1: 0.91807\n",
            "each epochs training time : 173m 20s\n",
            "\n",
            "\n",
            "Epoch 94/100\n",
            "----------\n",
            "train Loss: 0.06304 Acc: 98.32971 macro-f1: 0.93796\n",
            "vaild Loss: 0.13164 Acc: 96.02649 macro-f1: 0.90685\n",
            "each epochs training time : 175m 11s\n",
            "\n",
            "\n",
            "Epoch 95/100\n",
            "----------\n",
            "train Loss: 0.06568 Acc: 98.36312 macro-f1: 0.94263\n",
            "vaild Loss: 0.13369 Acc: 96.45501 macro-f1: 0.91693\n",
            "==> best model saved - 95 / 96.5\n",
            "each epochs training time : 177m 2s\n",
            "\n",
            "\n",
            "Epoch 96/100\n",
            "----------\n",
            "train Loss: 0.06206 Acc: 98.46334 macro-f1: 0.94659\n",
            "vaild Loss: 0.13889 Acc: 95.79275 macro-f1: 0.90902\n",
            "each epochs training time : 178m 52s\n",
            "\n",
            "\n",
            "Epoch 97/100\n",
            "----------\n",
            "train Loss: 0.06186 Acc: 98.36312 macro-f1: 0.94256\n",
            "vaild Loss: 0.14946 Acc: 95.48111 macro-f1: 0.90660\n",
            "each epochs training time : 180m 43s\n",
            "\n",
            "\n",
            "Epoch 98/100\n",
            "----------\n",
            "train Loss: 0.06113 Acc: 98.41323 macro-f1: 0.94604\n",
            "vaild Loss: 0.13186 Acc: 96.14336 macro-f1: 0.91444\n",
            "each epochs training time : 182m 34s\n",
            "\n",
            "\n",
            "Epoch 99/100\n",
            "----------\n",
            "train Loss: 0.06080 Acc: 98.51345 macro-f1: 0.94669\n",
            "vaild Loss: 0.13930 Acc: 95.94858 macro-f1: 0.91453\n",
            "each epochs training time : 184m 25s\n",
            "\n",
            "\n",
            "Training complete in 184m 25s\n",
            "Best valid Acc: 95 - 96.5\n",
            "model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 모델을 평가 모드로 변경하고 test 데이터 분류\n",
        "test_png = sorted(glob('/content/drive/MyDrive/dacon/dacon/open/test/*.png'))\n",
        "test_imgs = [img_load(n) for n in tqdm(test_png)]\n",
        "test_dataset = Custom_dataset_3(np.array(test_imgs), np.array([\"tmp\"]*len(test_imgs)), mode='test')\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "model.eval()\n",
        "f_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in (test_loader):\n",
        "        x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(x)\n",
        "        f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())"
      ],
      "metadata": {
        "id": "0ptPvuQJpneb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f1364e-5252-4704-ee49-9d3ffbc358a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2154/2154 [02:15<00:00, 15.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/dacon/dacon/model/EfficientNetV2_withsam.pt')"
      ],
      "metadata": {
        "id": "ukP3VHUsRAe-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 숫자로된 레이블을 문자열 레이블로 변경\n",
        "label_decoder = {value:key for key, value in label_unique.items()}\n",
        "f_result = [label_decoder[result] for result in f_pred]"
      ],
      "metadata": {
        "id": "PQ1GdXNMpncX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime as dt \n",
        "today = dt.today().strftime('%Y-%m-%d')\n",
        "version = f'efficientNetv2_rw_by_timm_imageTransfroms_ver_{today}'\n",
        "submission = pd.read_csv(f\"/content/drive/MyDrive/dacon/dacon/open/sample_submission.csv\")\n",
        "\n",
        "## 시각적 확인을 위해 문자열 레이블로 이루어진 된 label 필드 생성\n",
        "submission[\"label\"] = f_result\n",
        "display(submission)\n",
        "submission.to_csv(f\"/content/drive/MyDrive/dacon/dacon/open/efficientv2_rw_sam0507_{version}.csv\", index = False)"
      ],
      "metadata": {
        "id": "7ARPpkY0pnaB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "974b48ca-0d3f-4675-b61d-5b2341bb9c61"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      index             label\n",
              "0         0   tile-glue_strip\n",
              "1         1         grid-good\n",
              "2         2   transistor-good\n",
              "3         3  tile-gray_stroke\n",
              "4         4         tile-good\n",
              "...     ...               ...\n",
              "2149   2149  tile-gray_stroke\n",
              "2150   2150        screw-good\n",
              "2151   2151         grid-good\n",
              "2152   2152        cable-good\n",
              "2153   2153       zipper-good\n",
              "\n",
              "[2154 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-76377141-543b-4dd3-b320-7018cb0afbb2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tile-glue_strip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>grid-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>transistor-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>tile-gray_stroke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>tile-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2149</th>\n",
              "      <td>2149</td>\n",
              "      <td>tile-gray_stroke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2150</th>\n",
              "      <td>2150</td>\n",
              "      <td>screw-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2151</th>\n",
              "      <td>2151</td>\n",
              "      <td>grid-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2152</th>\n",
              "      <td>2152</td>\n",
              "      <td>cable-good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2153</th>\n",
              "      <td>2153</td>\n",
              "      <td>zipper-good</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2154 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76377141-543b-4dd3-b320-7018cb0afbb2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-76377141-543b-4dd3-b320-7018cb0afbb2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-76377141-543b-4dd3-b320-7018cb0afbb2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tXnn4mHxsr26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BukF9HhGpnXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CWrYs5a8pnVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7oYttUh1pnTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SPcMh6OlpnDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6775D6_dpnBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IxWngj1epm_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_ET-IKYUpm9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lECnllqupm7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l6_Vp7D8pm5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jNprf_IFpm22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "59IWivt_pm0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wYcKnbQapmHp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}